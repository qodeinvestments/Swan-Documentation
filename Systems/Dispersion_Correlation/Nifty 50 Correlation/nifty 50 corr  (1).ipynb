{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee474fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta='35' #set any delta value and run the entire notebook after setting your input and output paths\n",
    "leverage=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e943ae66",
   "metadata": {},
   "source": [
    "### Importing Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a674c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime as dt\n",
    "from datetime import datetime, date, time,timedelta\n",
    "import pandas as pd\n",
    "from csv import DictWriter\n",
    "import urllib\n",
    "import re,datetime\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "import re\n",
    "import  gc\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5502d84",
   "metadata": {},
   "source": [
    "### Importing Weights and creating the stocklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17163cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wgt_lot_path=r'C:\\Aman\\Nifty correlation\\Weights.csv'\n",
    "weight = pd.read_csv(wgt_lot_path,parse_dates = [\"date\"],dayfirst = True,usecols = [\"Security Symbol\",\"Weightage\",\"date\"])\n",
    "\n",
    "stock_list=pd.unique(weight['Security Symbol']).tolist()\n",
    "print(len(stock_list))\n",
    "for j in stock_list:\n",
    "    #removing the below stocks as they have incorrect data\n",
    "    if  j=='ADANIENT' or j=='CROMPGREAV' or j=='CROMPGREAV' or j=='GRASIM' or j=='CENTURYTEX' : \n",
    "        print(j)\n",
    "        stock_list.remove(j)\n",
    "        \n",
    "       \n",
    "for i in range(len(stock_list)):\n",
    "    if stock_list[i]=='BAJAJ-AUTO':\n",
    "        stock_list[i]=stock_list[i].replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0cd2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "############\n",
    "#INPUTS\n",
    "#############\n",
    "data=pd.read_csv(r'C:\\Aman\\Nifty correlation\\08-09\\eq_fill_check2.csv',parse_dates = [\"Nifty_Date_CE\"],dayfirst = True)\n",
    "output_path=r'C:\\Aman\\Nifty correlation\\nifty 50\\with missing stocks/'+delta+' delta'\n",
    "wgt_lot_path=r'C:\\Aman\\Nifty correlation\\Weights.csv'\n",
    "index = \"Nifty\"\n",
    "index_file_path = r\"C:\\Aman\\Nifty correlation\\nifty data\\delta greater than 50\\NIFTY-I.csv\"\n",
    "stfolder = r\"C:\\Aman\\Nifty correlation\\data with greeks\\05-50DeltaStrikesAdded\"\n",
    "    \n",
    "direc = r\"C:\\Aman\\Nifty correlation\\nifty 50\\with missing stocks\"\n",
    "\n",
    "index_lotsize = 1\n",
    "\n",
    "initial_equity = 100000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lookback_period = 15\n",
    "std_dev = 2\n",
    "\n",
    "#################\n",
    "#EXPIRY DATES\n",
    "####################\n",
    "symbols = stock_list.copy()\n",
    "symbols.append(index)\n",
    "\n",
    "exp_file_path = r\"C:\\Aman\\Nifty correlation\\expiry_dates_2022.csv\"\n",
    "exp_df = pd.read_csv(exp_file_path,parse_dates = [\"curr_exp_date\",\"Date\"],dayfirst =True,usecols = [\"curr_exp_date\",\"Date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011d9546",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02f1203",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generating the current month stock data files\n",
    "\n",
    "idx = pd.read_csv(index_file_path,parse_dates = [\"Date\"],dayfirst = True)\n",
    "df1 = idx[idx['Ticker'].str.endswith('-I')]\n",
    "\n",
    "index_file_path = direc + \"/Nifty.csv\"\n",
    "\n",
    "df1.to_csv(index_file_path)\n",
    "\n",
    "\n",
    "print(idx.shape[0])\n",
    "print(df1.shape[0])\n",
    "    \n",
    "for i in stock_list:\n",
    "    df = pd.read_csv(stfolder + '/' + i + \"-I.csv\",parse_dates = [\"Date\"],dayfirst = True)\n",
    "    df1 = df[df['Ticker'].str.endswith('-I')]\n",
    "\n",
    "    \n",
    "    path = direc + '//stdata/' + i + \".csv\"\n",
    "    \n",
    "    df1.to_csv(path)\n",
    "\n",
    "    \n",
    "    print(i,df.shape[0])\n",
    "    print(i,df1.shape[0])\n",
    "\n",
    "stfolder = direc\n",
    "print(index_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68880e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "stfolder = direc+\"/stdata\"\n",
    "index_file_path = direc + \"/Nifty.csv\"\n",
    "stfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990df0a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Separating out ATM CE and PE for stocks and Index - CURRENT month contracts\n",
    "from tqdm.notebook import tqdm\n",
    "stfolder = direc+\"/stdata\"\n",
    "\n",
    "idx = pd.read_csv(index_file_path,usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                    \"OPT_Close\",\"EQ_Close\",\"IV\",\"Delta_\"+delta+\"_Strike\"],parse_dates = [\"Date\"])\n",
    "\n",
    "idx=idx[~((idx['Date'] > '25-11-2021'))]\n",
    "\n",
    "idx = pd.merge(idx, exp_df, on = 'Date', how = 'left')\n",
    "\n",
    "idx_ce = idx[(idx[\"Strike\"] == idx[\"Delta_\"+delta+\"_Strike\"]) & (idx[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "idx_pe = idx[(idx[\"Strike\"] == idx[\"Delta_\"+delta+\"_Strike\"]) & (idx[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "\n",
    "\n",
    "final = idx_ce.merge(idx_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "\n",
    "final = final.T.drop_duplicates().T  #Drop Duplicates Columns\n",
    "\n",
    "final = final.add_prefix(index +\"_\")\n",
    "final = final.sort_values(by=index +\"_\"+'Date_CE')\n",
    "\n",
    "final.to_csv(output_path+\"/check-1data.csv\")\n",
    "\n",
    "#print(final)\n",
    "for i in tqdm(stock_list):\n",
    "    if i=='BAJAJ-AUTO':\n",
    "        i=i.replace('-','_')\n",
    "    print(i)\n",
    "    df1 = pd.read_csv(stfolder+ '/' + i + \".csv\",parse_dates = [\"Date\"],usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                    \"OPT_Close\",\"EQ_Close\",\"IV\",\"Delta_\"+delta+\"_Strike\"])\n",
    "\n",
    "\n",
    "    df1 = pd.merge(df1, exp_df, on = 'Date', how = 'left')\n",
    "    \n",
    "    \n",
    "    df_ce = df1[(df1[\"Strike\"] == df1[\"Delta_\"+delta+\"_Strike\"]) & (df1[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "    df_pe = df1[(df1[\"Strike\"] == df1[\"Delta_\"+delta+\"_Strike\"]) & (df1[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "    print(df1.shape[0], df_ce.shape[0], df_pe.shape[0])\n",
    "    temp = df_ce.merge(df_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "    temp = temp.T.drop_duplicates().T\n",
    "    temp = temp.add_prefix(i +\"_\")\n",
    "    \n",
    "    final = final.merge(temp,how='left',left_on=index + '_Date_CE', right_on= i + '_Date_CE')\n",
    "    \n",
    "final = final.T.drop_duplicates().T\n",
    "curr_final_df = final.copy()\n",
    "u = curr_final_df.select_dtypes(exclude=['datetime'])\n",
    "curr_final_df[u.columns] = u.fillna(0)\n",
    "\n",
    "curr_final_df.to_csv(output_path+\"/check-2data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a352bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=curr_final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6d08d",
   "metadata": {},
   "source": [
    "# Adding Weights Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfc0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight = pd.read_csv(wgt_lot_path,parse_dates = [\"date\"],dayfirst = True,usecols = [\"Security Symbol\",\"Weightage\",\"date\"])\n",
    "grp = weight.groupby(\"Security Symbol\")\n",
    "data[index + \"_OG_Weight\"] = 100\n",
    "for i in stock_list:\n",
    "    if i=='BAJAJ_AUTO':\n",
    "        i=i.replace('_','-')\n",
    "    d1 = grp.get_group(i)\n",
    "    if i=='BAJAJ-AUTO':\n",
    "        i=i.replace('-','_')\n",
    "    d1[i + \"_OG_Weight\"] = d1[\"Weightage\"]\n",
    "    d1[index+ \"_Date_CE\"] = d1[\"date\"]\n",
    "    data = pd.merge(data,d1[[index+ \"_Date_CE\",i + \"_OG_Weight\"]],on= index+ \"_Date_CE\", how='left')\n",
    "\n",
    "if data.isnull().sum().sum() != 0:\n",
    "    print(data.isnull().sum().sum())\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551d428",
   "metadata": {},
   "source": [
    "### Adding Final Weight Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81750a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stock_list)):\n",
    "    if stock_list[i]=='BAJAJ-AUTO':\n",
    "        stock_list[i]=stock_list[i].replace('-','_')\n",
    "\n",
    "filter_col = [col for col in data if col.endswith('Weight')]\n",
    "filter_col.remove(index + \"_OG_Weight\")\n",
    "\n",
    "data[\"actual_wgt_sum\"] = data[filter_col].sum(axis=1)\n",
    "for i in stock_list:\n",
    "    data[i + \"_Final_Weight\"] = data[i + \"_OG_Weight\"]/data[\"actual_wgt_sum\"]\n",
    "data[index + \"_Final_Weight\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "u = data.select_dtypes(exclude=['datetime'])\n",
    "data[u.columns] = u.fillna(0)\n",
    "\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476ebfc",
   "metadata": {},
   "source": [
    "### Adding entry and exit signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c961bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = data.copy()\n",
    "\n",
    "df1['correlation_entry'] = np.nan\n",
    "df1['correlation_exit'] = np.nan\n",
    "df1['bb_signal'] = np.nan\n",
    "\n",
    "df1['bb_signal'] = df1.apply(lambda x: 0 if x['Nifty_Date_CE']==x['Nifty_curr_exp_date_CE'] else 1, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if(i==0): #for the First trade\n",
    "        df1.loc[i,'correlation_entry'] = True\n",
    "        df1.loc[i,'correlation_exit'] = False\n",
    "        \n",
    "        #If previous day was expiry current day entry is set to true\n",
    "    elif (df1.loc[i-1,'Nifty_Date_CE'] == df1.loc[i-1,'Nifty_curr_exp_date_CE']):  \n",
    "        df1.loc[i,'correlation_entry'] = True\n",
    "        df1.loc[i,'correlation_exit'] = False\n",
    "        \n",
    "        #If CURRENT day IS expiry, current day Exit is set to true\n",
    "    elif (df1.loc[i,'Nifty_Date_CE'] == df1.loc[i,'Nifty_curr_exp_date_CE']):\n",
    "        df1.loc[i,'correlation_entry'] = False\n",
    "        df1.loc[i,'correlation_exit'] = True\n",
    "    \n",
    "    \n",
    "    else: # for The rest of the days\n",
    "        df1.loc[i,'correlation_entry'] = False\n",
    "        df1.loc[i,'correlation_exit'] = False\n",
    "\n",
    "display(df1)          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d448cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "\n",
    "\n",
    "df2.to_csv(output_path + '/with entry exit signals_all data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872aaef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df2[  (df2[\"correlation_exit\"] == True) | \n",
    "               (df2[\"correlation_entry\"] == True) ].reset_index()\n",
    "\n",
    "df.to_csv(output_path + '/with entry exit signals.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60746881",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(output_path + '/with entry exit signals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce88bd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(stock_list)):\n",
    "    if stock_list[i]=='BAJAJ-AUTO':\n",
    "        stock_list[i]=stock_list[i].replace('-','_')\n",
    "    \n",
    "#removing stocks which had their names changed\n",
    "stock_list.remove('HEROHONDA')\n",
    "stock_list.remove('RANBAXY')\n",
    "stock_list.remove('STER')\n",
    "stock_list.remove('SESAGOA')\n",
    "stock_list.remove('INFOSYSTCH')\n",
    "stock_list.remove('SSLT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107dc42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trades = []\n",
    "index='Nifty'\n",
    "for i in df.index[:-1]:\n",
    "    if df[\"bb_signal\"][i] == 1:\n",
    "        print(i)\n",
    "        strikes_ce = {}\n",
    "        strikes_pe = {}\n",
    "        strikes_ce[index] = df[index + \"_Strike_CE\"][i]\n",
    "        strikes_pe[index] = df[index + \"_Strike_PE\"][i] #While running for ATM, set \"_Strike_PE\" to \"_Strike_CE\"\n",
    "        for j in stock_list:\n",
    "            if j=='AREVAT_D' or j=='L_TFH' or j=='M_M' or j=='M_MFIN' :    \n",
    "                j=j.replace('_','&')\n",
    "\n",
    "            strikes_ce[j] = df[j+\"_Strike_CE\"][i]\n",
    "            if j+\"_Strike_PE\" not in df.columns:\n",
    "                     strikes_pe[j] = df[j+\"_Strike_CE\"][i]\n",
    "            else:\n",
    "                strikes_pe[j] = df[j+\"_Strike_PE\"][i]\n",
    "        trades.append( \n",
    "                        { \n",
    "                          \"entry_date\": df[index + \"_Date_CE\"][i],\n",
    "                          \"exit_date\" : df[index + \"_Date_CE\"][i+1],\n",
    "                          \"expiry_date\" : df[index+\"_curr_exp_date_CE\"][i],\n",
    "                          \"type_of_trade\": \"LISS\",\n",
    "                          \"strike_ce\" : strikes_ce,\n",
    "                            \"strike_pe\" : strikes_pe\n",
    "                        }\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac20e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(trades):\n",
    "    date1 = i[\"entry_date\"]\n",
    "    date2 = i[\"exit_date\"]\n",
    "\n",
    "    expiry = i[\"expiry_date\"]\n",
    "    print(date1)\n",
    "    print(date2)\n",
    "\n",
    "    if expiry == date1:\n",
    "        df = pd.read_csv(index_file_path,parse_dates = [\"Date\"])\n",
    "\n",
    "        df = df[(df[\"Date\"] > date1) & (df[\"Date\"] <= date2) & (df[\"Strike\"] == i[\"strike_ce\"][index])]\n",
    "\n",
    "        df_dict = {}\n",
    "        df_dict[index] = nextdf\n",
    "\n",
    "        for j in stock_list:\n",
    "            if j=='AREVAT_D' or j=='L_TFH' or j=='M_M' or j=='M_MFIN' :    \n",
    "                j=j.replace('_','&')\n",
    "            path = stfolder + '/' + j + \"-I.csv\"\n",
    "            df = pd.read_csv(path,parse_dates = [\"Date\"])\n",
    "            print(df)\n",
    "\n",
    "            df = df[((df[\"Date\"] >= date1) & (df[\"Date\"] <= date2)) & ((df[\"Strike\"] == i[\"strike_ce\"][j]) | (df[\"Strike\"] == i[\"strike_pe\"][j]))]\n",
    "\n",
    "        i[\"dataframes\"] = df_dict\n",
    "\n",
    "    else:\n",
    "        df = pd.read_csv(index_file_path,parse_dates = [\"Date\"])\n",
    "        a=df[((df[\"Date\"] >= date1) & (df[\"Date\"] <= date2)) & ((df[\"Strike\"] == i[\"strike_ce\"][index]) ) & (df['Option_Type']=='CE')]#.add_suffix(\"_CE\")\n",
    "        b=df[((df[\"Date\"] >= date1) & (df[\"Date\"] <= date2)) & ((df[\"Strike\"] == i[\"strike_pe\"][index])  & (df['Option_Type']=='PE'))]#.add_suffix(\"_PE\")\n",
    "        df=a.append(b)\n",
    "        df = pd.merge(df, exp_df, on = 'Date', how = 'left')\n",
    "        df_dict = {}\n",
    "        df_dict[index] = df\n",
    "\n",
    "        for j in stock_list:\n",
    "            if j=='AREVAT_D' or j=='L_TFH' or j=='M_M' or j=='M_MFIN' :    \n",
    "                    j=j.replace('_','&')\n",
    "            df = pd.read_csv(stfolder + '/' + j + \".csv\",parse_dates = [\"Date\"])\n",
    "            x=df[((df[\"Date\"] >= date1) & (df[\"Date\"] <= date2)) & ((df[\"Strike\"] == i[\"strike_ce\"][j]) ) & (df['Option_Type']=='CE')]#.add_suffix(\"_CE\")\n",
    "            y=df[((df[\"Date\"] >= date1) & (df[\"Date\"] <= date2)) & ((df[\"Strike\"] == i[\"strike_pe\"][j])  & (df['Option_Type']=='PE'))]#.add_suffix(\"_PE\")\n",
    "            df=x.append(y)\n",
    "            \n",
    "            df = pd.merge(df, exp_df, on = 'Date', how = 'left')\n",
    "            \n",
    "            df_dict[j] = df\n",
    "        i[\"dataframes\"] = df_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ffb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Separating CE and PE in every dataframe to format into coloumns\n",
    "\n",
    "result = pd.DataFrame()\n",
    "for trade_dict in tqdm(trades):\n",
    "    df_dict = trade_dict[\"dataframes\"]\n",
    "    entry = trade_dict[\"entry_date\"]\n",
    "    exit = trade_dict[\"exit_date\"]\n",
    "    \n",
    "    idx = df_dict[index][[\"Date\",\"Ticker\",\"OPT_Close\",\"Strike\",\"Option_Type\",\"IV\",\"EQ_Close\",\"curr_exp_date\"]]\n",
    "    \n",
    "    idx_ce = idx[(idx[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "    idx_pe = idx[(idx[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "\n",
    "    final = idx_ce.merge(idx_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "    final = final.add_prefix(index +\"_\")\n",
    "    \n",
    "    for i in df_dict:\n",
    "        if i == index:\n",
    "            continue\n",
    "        print(i)\n",
    "        df1 = df_dict[i][[\"Date\",\"Ticker\",\"OPT_Close\",\"Strike\",\"Option_Type\",\"IV\",\"EQ_Close\",\"curr_exp_date\"]]\n",
    "        df_ce = df1[(df1[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "        df_pe = df1[(df1[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "        df_ce = df_ce.add_prefix(i +\"_\")\n",
    "        df_pe = df_pe.add_prefix(i +\"_\")\n",
    "\n",
    "        final = final.merge(df_ce,left_on=index + '_Date_CE' , right_on=i + '_Date_CE', how = 'left')\n",
    "        final = final.merge(df_pe,left_on=index + '_Date_CE' , right_on= i +'_Date_PE', how = 'left').drop(columns = [i +\"_\"+\"Option_Type_PE\",i +\"_\"+\"Option_Type_CE\"])\n",
    "        \n",
    "\n",
    "    final[\"Trade_Type\"] = trade_dict[\"type_of_trade\"]\n",
    "    \n",
    "    conditions = [\n",
    "    (final[index + '_Date_CE'] == entry),\n",
    "    (final[index + '_Date_CE'] == exit),\n",
    "    (final[index + '_Date_CE'] != exit) & (final[index + '_Date_CE'] != entry)]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    values = ['F', 'L', 'M']\n",
    "\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    final['D_marker'] = np.select(conditions, values)\n",
    "    result = result.append(final)\n",
    "    \n",
    "result = result.T.drop_duplicates().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f48670",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=result.drop_duplicates()#.to_csv(output_path+'/datacheck2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acecb76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.to_csv(output_path+'/datacheck1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = result.copy()\n",
    "display(data)\n",
    "data.to_csv(output_path + '/before_weigt_add_second_time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea0a8f",
   "metadata": {},
   "source": [
    "### Adding Weight and Lot Columns Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afe0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = pd.read_csv(wgt_lot_path,parse_dates = [\"date\"],dayfirst = True,usecols = [\"Security Symbol\",\"Weightage\",\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62b429",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(output_path+'/datacheck1.csv',parse_dates=[\"Nifty_Date_CE\"])\n",
    "weight = pd.read_csv(wgt_lot_path,parse_dates = [\"date\"],dayfirst = True,usecols = [\"Security Symbol\",\"Weightage\",\"date\"])\n",
    "grp = weight.groupby(\"Security Symbol\")\n",
    "data[index + \"_OG_Weight\"] = 100\n",
    "data[index + \"_Lotsize\"] = index_lotsize\n",
    "for i in tqdm(stock_list):\n",
    "    data[i+'_EQ_Close_PE']=data[i+'_EQ_Close_CE']\n",
    "    print(i)\n",
    "    if i=='BAJAJ_AUTO':\n",
    "         i=i.replace('_','-')\n",
    "    d1 = grp.get_group(i)\n",
    "    if i=='BAJAJ-AUTO':\n",
    "         i=i.replace('-','_')\n",
    "    d1[i + \"_OG_Weight\"] = d1[\"Weightage\"]\n",
    "    d1[i + \"_Lotsize\"] = 1 #d1[\"Lotsize\"]\n",
    "    d1[index+ \"_Date_CE\"] = d1[\"date\"]\n",
    "    data = pd.merge(data,d1[[index+ \"_Date_CE\",i + \"_OG_Weight\",i+'_Lotsize']],on= index+ \"_Date_CE\", how='left')\n",
    "    \n",
    "if data.isnull().sum().sum() != 0:\n",
    "    print(data.isnull().sum().sum())\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "\n",
    "u = data.select_dtypes(exclude=['datetime'])\n",
    "data[u.columns] = u.fillna(0)\n",
    "\n",
    "display(data)\n",
    "\n",
    "data.to_csv(output_path + '/weigt_add_second_time1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d041c8",
   "metadata": {},
   "source": [
    "### Adding Final Weight Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in data if col.endswith('Weight')]\n",
    "filter_col.remove(index + \"_OG_Weight\")\n",
    "\n",
    "data[\"actual_wgt_sum\"] = data[filter_col].sum(axis=1)\n",
    "for i in stock_list:\n",
    "    if i=='BAJAJ-AUTO':\n",
    "         i=i.replace('-','_')\n",
    "    data[i + \"_Final_Weight\"] = data[i + \"_OG_Weight\"]/data[\"actual_wgt_sum\"]\n",
    "data[index + \"_Final_Weight\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in data if col.endswith('Final_Weight')]\n",
    "filter_col.remove(index + \"_Final_Weight\")\n",
    "print(filter_col)\n",
    "\n",
    "data[filter_col] = data[filter_col].replace(0,np.nan)\n",
    "\n",
    "data[\"Min_Share_Value\"] = data[filter_col].min(axis = 1)\n",
    "data[\"Min_Share_Name\"] = data[filter_col].idxmin(axis=1)\n",
    "print(data[\"Min_Share_Value\"][250])\n",
    "print(data[\"Min_Share_Name\"][250])\n",
    "\n",
    "data[filter_col] = data[filter_col].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da097072",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(data)\n",
    "data.to_csv(output_path + '/before_qty_add 1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45cf374",
   "metadata": {},
   "source": [
    "### Filling in data holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105ebf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "index=\"Nifty\"\n",
    "result=pd.read_csv(output_path + '/before_qty_add 1.csv')\n",
    "start_dates=[]\n",
    "end_dates=[]\n",
    "for idi,row in result.iterrows():\n",
    "    if result.loc[idi,'D_marker']=='F':\n",
    "        start_dates.append(result.loc[idi,'Nifty_Date_CE'])\n",
    "    if result.loc[idi,'D_marker']=='L':\n",
    "        end_dates.append(result.loc[idi,'Nifty_Date_CE'])\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "final=pd.DataFrame()\n",
    "l1=[\"WIPRO\"]\n",
    "for i in tqdm(range(len(start_dates))):\n",
    "\n",
    "    start=start_dates[i]\n",
    "    end=end_dates[i]\n",
    "\n",
    "    temp=result[(result['Nifty_Date_CE']>=start) &\n",
    "                (result['Nifty_Date_CE']<=end) ]\n",
    "    temp=temp.reset_index(drop=True)\n",
    "    temp.fillna(0,inplace=True)\n",
    "    for stock in tqdm(stock_list):\n",
    "        \n",
    "        if stock=='AREVAT_D' or stock=='L_TFH'  or stock=='M_M' or stock=='M_MFIN' : \n",
    "                    stock=stock.replace('_','&')\n",
    "        close=temp[stock+'_EQ_Close_CE'].iat[0]\n",
    "        date=temp[index+'_Date_CE'].iat[0]\n",
    "        strike_ce=0\n",
    "\n",
    "        ticker_ce=0\n",
    "        ticker_pe=0\n",
    "        exp_date=0\n",
    "        close_ce=0\n",
    "        close_pe=0\n",
    "        iv=0\n",
    "        if temp[stock+'_EQ_Close_CE'].iat[0]>0:\n",
    "            close=temp[stock+'_EQ_Close_CE'].iat[0]\n",
    "            date=temp[index+'_Date_CE'].iat[0]\n",
    "            ticker_ce=temp[stock+'_Ticker_CE'].iat[0]\n",
    "            close_ce=temp[stock+'_OPT_Close_CE'].iat[0]\n",
    "            iv=temp[stock+'_IV_CE'].iat[0]\n",
    "            for idi,row in temp.iterrows():\n",
    "                if temp.loc[idi,stock+'_EQ_Close_CE']>0:\n",
    "                    close=temp.loc[idi,stock+'_EQ_Close_CE']\n",
    "                    date=temp.loc[idi,index+'_Date_CE']\n",
    "                    ticker_ce=temp.loc[idi,stock+'_Ticker_CE']\n",
    "                    close_ce=temp.loc[idi,stock+'_OPT_Close_CE']\n",
    "                    iv=temp.loc[idi,stock+'_IV_CE']\n",
    "\n",
    "                if temp.loc[idi,stock+'_EQ_Close_CE']==0:\n",
    "                    if temp.loc[idi,'D_marker']=='L':\n",
    "                        print(idi,temp.loc[idi,'Nifty_Date_CE'],stock)\n",
    "                    temp.loc[idi,stock+'_EQ_Close_CE']=close\n",
    "                    temp.loc[idi,stock+'_Date_CE']=date\n",
    "                    temp.loc[idi,stock+'_Ticker_CE']=ticker_ce\n",
    "                    temp.loc[idi,stock+'_OPT_Close_CE']=close_ce\n",
    "                if temp.loc[idi,stock+'_Strike_CE']==0:\n",
    "                    temp.loc[idi,stock+'_IV_CE']=iv\n",
    "        \n",
    "        if temp[stock+'_Strike_PE'].iat[0]>0:\n",
    "            iv=temp[stock+'_IV_PE'].iat[0]\n",
    "            close=temp[stock+'_EQ_Close_PE'].iat[0]\n",
    "            date=temp[index+'_Date_CE'].iat[0]\n",
    "            ticker_pe=temp[stock+'_Ticker_PE'].iat[0]\n",
    "            close_pe=temp[stock+'_OPT_Close_PE'].iat[0]\n",
    "            for idi,row in temp.iterrows():\n",
    "                if temp.loc[idi,stock+'_Strike_PE']>0:\n",
    "                    iv=temp.loc[idi,stock+'_IV_PE']\n",
    "                    close=temp.loc[idi,stock+'_EQ_Close_CE']\n",
    "                    date=temp.loc[idi,index+'_Date_CE']\n",
    "                    ticker_pe=temp.loc[idi,stock+'_Ticker_PE']\n",
    "                    close_pe=temp.loc[idi,stock+'_OPT_Close_PE']\n",
    "\n",
    "\n",
    "                if temp.loc[idi,stock+'_Strike_PE']==0:\n",
    "                    if temp.loc[idi,'D_marker']=='L':\n",
    "                        print(idi,temp.loc[idi,'Nifty_Date_CE'],stock)\n",
    "                    temp.loc[idi,stock+'_IV_PE']=iv\n",
    "                    temp.loc[idi,stock+'_EQ_Close_CE']=close\n",
    "                    temp.loc[idi,stock+'_Date_CE']=date\n",
    "                    temp.loc[idi,stock+'_Ticker_PE']=ticker_pe\n",
    "                    temp.loc[idi,stock+'_OPT_Close_PE']=close_pe\n",
    "                if temp.loc[idi,stock+'_Strike_PE']==0:\n",
    "                    temp.loc[idi,stock+'_IV_PE']=iv\n",
    "    final=final.append(temp)\n",
    "            \n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e53077",
   "metadata": {},
   "source": [
    "### Adding IV's and Implied correlation ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "final[index + \"_Avg IV\"] = (final[index + \"_IV_CE\"] + final[index + \"_IV_PE\"])/2\n",
    "for i in tqdm(stock_list):\n",
    "    final[i + \"_Avg IV\"] = (final[i + \"_IV_CE\"] + final[i + \"_IV_PE\"])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff8da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "final['stock_iv']=0\n",
    "for i in stock_list:\n",
    "    final['stock_iv'] += final[i + '_Avg IV']*final[i + \"_Final_Weight\"] \n",
    "    \n",
    "final['implied_correl'] = (final[index + \"_Avg IV\"]/final['stock_iv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66de5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(output_path + '/IV fill check3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61971687",
   "metadata": {},
   "source": [
    "### Set expiry day option closing to intrinsic value or 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb3cd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result=pd.read_csv(output_path + '/IV fill check3.csv')\n",
    "from tqdm.notebook import tqdm\n",
    "stock_list2=['PNB']\n",
    "for i in tqdm(stock_list):\n",
    "    print(i)\n",
    "\n",
    "    if i=='M&M':\n",
    "        i='M_M'\n",
    "    df=pd.read_csv(r'C:\\Aman\\final updated equity data/'+i+'.csv',parse_dates = [\"Date\"],usecols = [\"Date\",'Close']).reset_index(drop=True)#inplace=True)\n",
    "    if i=='M_M':\n",
    "        i='M&M'\n",
    "\n",
    "    for idi, row in result.iterrows():\n",
    "        date=result.loc[idi,'Nifty_Date_CE']\n",
    "        if len(df[df['Date']==date]['Close']):\n",
    "            result.loc[idi,i+'_EQ_Close_CE']=df[df['Date']==date]['Close'].iat[0]\n",
    "            close=float(result.loc[idi,i+'_EQ_Close_CE'])\n",
    "        if  i!='TATACHEM' and i!='ADANIENT' and i!='GRASIM' and i!='CENTURYTEX': \n",
    "            if result.loc[idi,'D_marker']=='L':\n",
    "                if result.loc[idi,i+ \"_Ticker_CE\"]!='0' and result.loc[idi,i+ \"_Ticker_CE\"]!=0:\n",
    "                    ce_strike=float(result.loc[idi,i+ \"_Ticker_CE\"].replace(i,\"\").replace('CE-I',\"\"))\n",
    "                    pe_strike=float(result.loc[idi,i+ \"_Ticker_PE\"].replace(i,\"\").replace('PE-I',\"\"))\n",
    "                    if ce_strike>close:\n",
    "                        result.loc[idi,i+'_OPT_Close_CE']=0.05\n",
    "                    if pe_strike<close:\n",
    "                        result.loc[idi,i+'_OPT_Close_PE']=0.05\n",
    "                    if ce_strike<close:\n",
    "                        print(idi,i,'ce',result.loc[idi,i+'_OPT_Close_CE'],close,ce_strike)\n",
    "                        result.loc[idi,i+'_OPT_Close_CE']=close-ce_strike\n",
    "                        print(result.loc[idi,i+'_OPT_Close_CE'])\n",
    "                    if pe_strike>close:\n",
    "                        print(idi,i,'pe',result.loc[idi,i+'_OPT_Close_PE'],close,pe_strike)\n",
    "                        result.loc[idi,i+'_OPT_Close_PE']=pe_strike-close\n",
    "                        print(result.loc[idi,i+'_OPT_Close_PE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34743f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(output_path + '/eq fill with adj 1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1408ec0",
   "metadata": {},
   "source": [
    "### ADDING QTYS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3072b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.read_csv(output_path + '/eq fill with adj 1.csv')\n",
    "min_exp = []\n",
    "for idi, row in result.iterrows():\n",
    "    min_share_stock = row['Min_Share_Name'].replace(\"_Final_Weight\",\"\")\n",
    "    value  = row[min_share_stock + \"_EQ_Close_CE\"]*row[min_share_stock + \"_Lotsize\"]\n",
    "    min_exp.append(value)\n",
    "    \n",
    "result[\"MinExp\"] = min_exp\n",
    "#Calculating index exposure from minShareExposure\n",
    "result[index + \"_exp\"] = (result[\"MinExp\"]/result[\"Min_Share_Value\"])\n",
    "\n",
    "#calculating qty for index from exposure\n",
    "result[index + \"_lot\"] = (result[index + \"_exp\"]/(result[index + \"_Lotsize\"]*result[index + \"_EQ_Close_CE\"]))\n",
    "result[index + \"_qty\"] = result[index + \"_lot\"]*result[index + \"_Lotsize\"]\n",
    "\n",
    "result.to_csv(output_path + '/resultt.csv')\n",
    "\n",
    "#for stocks with includeok=0, making '_Lotsize' & '_EQ_Close_CE' non-zero\n",
    "\n",
    "filter_col_Lotsize = [col for col in result if col.endswith('_Lotsize')]\n",
    "filter_col_Lotsize.remove(index + \"_Lotsize\")\n",
    "print(filter_col_Lotsize)\n",
    "result[filter_col_Lotsize] = result[filter_col_Lotsize].replace(0,1)\n",
    "\n",
    "\n",
    "filter_col_EQ_Close = [col for col in result if col.endswith('_EQ_Close_CE')]\n",
    "filter_col_EQ_Close.remove(index + \"_EQ_Close_CE\")\n",
    "print(filter_col_EQ_Close)\n",
    "result[filter_col_EQ_Close] = result[filter_col_EQ_Close].replace(0,1)\n",
    "\n",
    "result['index_exp'] = result[index + \"_exp\"]\n",
    "\n",
    "result.to_csv(output_path + '/resulttcheckcheck.csv')\n",
    "\n",
    "result['stock_exp'] = 0\n",
    "\n",
    "for i in stock_list:\n",
    "    print(i)\n",
    "    if(i==min_share_stock):\n",
    "        result[index + \"_exp\"]\n",
    "        result[i + \"_Final_Weight\"]\n",
    "        result[i + \"_exp\"] = result[\"MinExp\"]\n",
    "        result[i  + \"_lot\"] = 1\n",
    "        result[i + \"_qty\"] = result[i + \"_lot\"]*result[i + \"_Lotsize\"]\n",
    "        \n",
    "        result['stock_exp'] += result[i + \"_exp\"]\n",
    "        \n",
    "    else:\n",
    "        result[index + \"_exp\"]\n",
    "        result[i + \"_Final_Weight\"]\n",
    "        result[i + \"_exp\"] = (result[index + \"_exp\"]*result[i + \"_Final_Weight\"])\n",
    "        result[i  + \"_lot\"] = (result[i + \"_exp\"]/(result[i + \"_Lotsize\"]*result[i+ \"_EQ_Close_CE\"]))\n",
    "        result[i + \"_qty\"] = result[i + \"_lot\"]*result[i + \"_Lotsize\"]\n",
    "        \n",
    "        result['stock_exp'] += result[i + \"_exp\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1475b15e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result.to_csv(output_path + '/qtycheckfinal 1.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb88a7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result['total_exposure'] = (result['index_exp'] + result['stock_exp'])*2#(*2 for CE and PE)\n",
    "\n",
    "conditions = [\n",
    "    (result[\"Trade_Type\"] == \"LISS\"),\n",
    "    (result[\"Trade_Type\"] == \"SILS\")]\n",
    "\n",
    "\n",
    "=\n",
    "values = [result['total_exposure']/leverage,result['total_exposure']/leverage]\n",
    "result['margin'] = np.select(conditions, values)\n",
    "\n",
    "result['units'] = (initial_equity/result['margin']).astype(int)\n",
    "\n",
    "result.to_csv(output_path+\"/final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(output_path+\"/final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (result[\"D_marker\"] == \"F\"),\n",
    "    (result[\"D_marker\"] != \"F\")]\n",
    "\n",
    "values = [result[index + \"_qty\"]*result['units'],np.nan]\n",
    "result[index + '_qty_final'] = np.select(conditions, values)\n",
    "\n",
    "for i in stock_list:\n",
    "    values = [result[i + \"_qty\"]*result['units'],np.nan]\n",
    "    result[i + '_qty_final'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9203b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5458d7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(output_path+\"/finalfinal1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04ce6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59cad0a",
   "metadata": {},
   "source": [
    "### Calculating PnL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c8dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    result=pd.read_csv(r\"C:\\Aman\\Nifty correlation\\nifty 50\\with missing stocks\\5 delta\"+\"/finalfinal1.csv\")\n",
    "    print(\"Calculating PnL.....\")\n",
    "    print(\"          \", datetime.datetime.now())\n",
    "    index=\"Nifty\"\n",
    "    u = result.select_dtypes(exclude=['datetime'])\n",
    "    result[u.columns] = u.fillna(0)\n",
    "\n",
    "    initial_equity=100000\n",
    "    plvalue = []\n",
    "    pl = 0\n",
    "    transaction_cost = 0\n",
    "\n",
    "    for idi, row in tqdm(result.iterrows()):\n",
    "        result.loc[idi,'stocks_plvalue']=0\n",
    "\n",
    "        pval = 0\n",
    "        if row[\"D_marker\"] == \"F\":\n",
    "                        \n",
    "            \n",
    "            result.loc[idi, 'pl_check'] = pl + initial_equity\n",
    "            result.loc[idi+1:, 'pl_check'] = np.nan\n",
    "            \n",
    "            (result.loc[idi ,'available_margin']) = (result.loc[idi ,'pl_check'])\n",
    "\n",
    "            result.loc[idi , 'units_new'] = (((result.loc[idi ,'pl_check']))/result.loc[idi, 'margin'])\n",
    "            result.loc[idi+1: , 'units_new'] = np.nan\n",
    "\n",
    "            result.loc[idi, index + '_qty_final'] = result.loc[idi, index + \"_qty\"] * result.loc[idi, 'units_new']\n",
    "            result.loc[idi+1:, index + '_qty_final'] = np.nan\n",
    "\n",
    "            result.loc[idi, 'plvalue_' + index] = 0\n",
    "            \n",
    "            contract_value = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi, index + \"_OPT_Close_CE\"] + result.loc[idi, index + \"_OPT_Close_PE\"])\n",
    "            \n",
    "            \n",
    "            for i in stock_list:     \n",
    "                    result.loc[idi, i + '_qty_final'] = result.loc[idi, i + \"_qty\"] * result.loc[idi, 'units_new']                    \n",
    "                    result.loc[idi+1:, i + '_qty_final'] = np.nan\n",
    "\n",
    "                    result.loc[idi, 'plvalue_' + i] = 0\n",
    "\n",
    "                    contract_value += result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_OPT_Close_CE\"] + result.loc[idi, i + \"_OPT_Close_PE\"])\n",
    "\n",
    "                    \n",
    "            result.loc[idi, 'transaction_cost'] = 0\n",
    "            result = result.ffill()\n",
    "        else:\n",
    "            if row[\"Trade_Type\"] == \"LISS\":\n",
    "                plvalce = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi, index + \"_OPT_Close_CE\"] - result.loc[idi-1, index + \"_OPT_Close_CE\"])\n",
    "                plvalpe = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi, index + \"_OPT_Close_PE\"] - result.loc[idi-1, index + \"_OPT_Close_PE\"])\n",
    "                pval = plvalce + plvalpe\n",
    "\n",
    "                result.loc[idi, 'plvalue_' + index] = plvalce + plvalpe\n",
    "\n",
    "                for i in stock_list:\n",
    "                    \n",
    "                        plvalce = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_OPT_Close_CE\"] - result.loc[idi, i + \"_OPT_Close_CE\"])\n",
    "                        plvalpe = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_OPT_Close_PE\"] - result.loc[idi, i + \"_OPT_Close_PE\"])\n",
    "                        pval += plvalce + plvalpe\n",
    "                        result.loc[idi,'stocks_plvalue']=result.loc[idi,'stocks_plvalue']+plvalce + plvalpe \n",
    "\n",
    "                        result.loc[idi, 'plvalue_' + i] = plvalce + plvalpe \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            pl += pval\n",
    "\n",
    "\n",
    "        if row['D_marker'] == \"L\":\n",
    "\n",
    "            contract_value += result.loc[idi-1, index + \"_qty_final\"]*(result.loc[idi-1, index + \"_OPT_Close_CE\"] + result.loc[idi-1, index + \"_OPT_Close_PE\"])\n",
    "\n",
    "            for i in stock_list:                                                                    \n",
    "                contract_value += result.loc[idi-1, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_OPT_Close_CE\"] + result.loc[idi-1, i + \"_OPT_Close_PE\"])\n",
    "\n",
    "            transaction_cost = contract_value*0.005\n",
    "            pl = pl - transaction_cost\n",
    "            result.loc[idi, 'transaction_cost'] = transaction_cost\n",
    "\n",
    "        plvalue.append(pval)\n",
    "\n",
    "    result[\"Daily_PL\"] = plvalue\n",
    "    result.loc[0,'Daily_PL'] = initial_equity\n",
    "    result[\"PortfolioValue\"] = (result[\"Daily_PL\"] - result[\"transaction_cost\"]).cumsum()\n",
    "    result.loc[0,'Daily_PL'] = 0\n",
    "\n",
    "    display(result)\n",
    "    \n",
    "    print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1edff67",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv(output_path+\"/pl1 \"+delta+\".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404031e",
   "metadata": {},
   "source": [
    "## Plotting the equity curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e52926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(28,17))\n",
    "plt.title('Equity Curve')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('PortfolioValue')\n",
    "plt.plot((result[\"Nifty_Date_CE\"]).astype('str'),result['PortfolioValue'], c = 'blue')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3341912",
   "metadata": {},
   "source": [
    "### Generating the Checksheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2d3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#result=pd.read_csv(r'C:\\Aman\\Nifty correlation\\07-09/pl_Test1.csv')\n",
    "finallist = pd.DataFrame()\n",
    "index='Nifty'\n",
    "temp1 = pd.DataFrame()\n",
    "tradelist = result[ (result[\"D_marker\"] == \"F\") | (result[\"D_marker\"] == \"L\") ]\n",
    "\n",
    "if list(tradelist.iloc[[-1]][\"D_marker\"])[0] == \"F\":\n",
    "    tradelist.drop(tradelist.tail(1).index,inplace=True)\n",
    "else:\n",
    "    tradelist = tradelist\n",
    "\n",
    "tr1 = tradelist[tradelist[\"D_marker\"]==\"F\"].reset_index().drop(columns = [\"index\"])\n",
    "tr2 = tradelist[tradelist[\"D_marker\"]==\"L\"].reset_index().drop(columns = [\"index\"])\n",
    "mdict = {}\n",
    "print(tr1.shape[0])\n",
    "if tr1.shape[0] == tr2.shape[0]:\n",
    "    for idx in tr1.index:\n",
    "        \n",
    "        \n",
    "\n",
    "        for i in stock_list:\n",
    "            mdict = {}\n",
    "            #print(i)\n",
    "            for j in [\"CE\",\"PE\"]:\n",
    "                mdict['Name']=i#.append(i)\n",
    "                mdict[\"Entry_Date\"] = tr1[index+ \"_Date_CE\"][idx]\n",
    "                mdict[\"Exit_Date\"] = tr2[index+ \"_Date_CE\"][idx]\n",
    "                mdict[\"EQ_at_entry\"] = tr1[i+ \"_EQ_Close_CE\"][idx]\n",
    "                mdict[\"EQ_at_exit\"] = tr2[i+ \"_EQ_Close_CE\"][idx]\n",
    "                \n",
    "                if tr1[i+ \"_Ticker_CE\"][idx]!='0' and tr1[i+ \"_Ticker_CE\"][idx]:\n",
    "                    mdict[\"Ticker_CE\"] = tr1[i+ \"_Ticker_CE\"][idx].replace(i,\"\").replace('CE-I',\"\")\n",
    "                    mdict[\"_Ticker_PE\"] = tr2[i+ \"_Ticker_PE\"][idx].replace(i,\"\").replace('PE-I',\"\")\n",
    "                else:\n",
    "                    mdict[\"Ticker_CE\"] = 0\n",
    "                    mdict[\"_Ticker_PE\"] = 0\n",
    "    \n",
    "\n",
    "                \n",
    "                mdict[ \"_Entry_Price_\" + j] = tr1[i + \"_OPT_Close_\" + j][idx]\n",
    "                \n",
    "                mdict[\"_Exit_Price_\" + j] = tr2[i + \"_OPT_Close_\" + j][idx]\n",
    "                mdict[\"_Qty_final\"] = tr1[i + \"_qty_final\"][idx]\n",
    "                mdict[\"Exposure\"] = tr1[i + \"_exp\"][idx]\n",
    "                mdict[\"P/L\"] = tr1[ \"plvalue_\"+i][idx]\n",
    "            temp = pd.DataFrame([mdict])\n",
    "            temp1=temp1.append(temp)\n",
    "        #display(temp)\n",
    "    finallist = finallist.append(temp1)\n",
    "else:\n",
    "    print(\"Error, Tradelist not proper\")\n",
    "\n",
    "finallist.reset_index().drop(columns = [\"index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710db0c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c047c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finallist.to_csv(output_path+'/final check2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3827855",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate CAGR \n",
    "result=pd.read_csv(r\"C:\\Aman\\Nifty correlation\\nifty 50\\with missing stocks\\45 delta\"+\"/pl1 45.csv\")\n",
    "df = result.copy()\n",
    "from pyxirr import xirr\n",
    "\n",
    "firstValue = round(df.loc[0, 'PortfolioValue'],4)\n",
    "lastValue = round(df.loc[df.index[-1], 'PortfolioValue'],4)\n",
    "\n",
    "firstDate = (df.loc[0, 'Nifty_Date_CE'])\n",
    "lastDate = (df.loc[df.index[-1], 'Nifty_Date_CE'])\n",
    "\n",
    "dates = [firstDate, lastDate]\n",
    "amounts = [-firstValue, lastValue]\n",
    "\n",
    "xirr = xirr(dates, amounts)\n",
    "\n",
    "print(xirr)\n",
    "### Calculate Daily Drawdown\n",
    "\n",
    "Roll_Max = round(df['PortfolioValue'].expanding().max(), 2)\n",
    "Daily_Drawdown = (round(df['PortfolioValue'], 2)/Roll_Max) - 1.0\n",
    "\n",
    "\n",
    "#Roll_Max[0:50], Daily_Drawdown[0:50]\n",
    "\n",
    "df['Daily_Drawdown'] = Daily_Drawdown * 100\n",
    "\n",
    "print(\"max dd\", min(df['Daily_Drawdown']))\n",
    "print(100*xirr/-min(df['Daily_Drawdown']))\n",
    "df.to_csv(output_path + '/DailyDrawdown.csv', index=False)\n",
    "\n",
    "### Monthly PNL Percentage \n",
    "\n",
    "df['Nifty_Date_CE'] = pd.to_datetime(df['Nifty_Date_CE'], dayfirst=True)\n",
    "\n",
    "df['Year'] = pd.DatetimeIndex(df['Nifty_Date_CE']).year\n",
    "df['Month'] = pd.DatetimeIndex(df['Nifty_Date_CE']).month\n",
    "#df.to_csv(output_path + '/df.csv', index=False)\n",
    "\n",
    "i = 0\n",
    "dfg = df.groupby(['Year', 'Month'])\n",
    "for name, group in dfg:\n",
    "    #print(name)\n",
    "    if i == 0: \n",
    "        firstValue = group['PortfolioValue'].iloc[0]\n",
    "        i = 1\n",
    "    else:\n",
    "        firstValue = lastValue\n",
    "\n",
    "    lastValue = group['PortfolioValue'].loc[group.index[-1]]\n",
    "    change = lastValue/firstValue - 1\n",
    "    df.loc[group.index, 'Change_%_Monthly'] = round(change * 100, 2)\n",
    "    \n",
    "i = 0\n",
    "dfg = df.groupby(['Year'])\n",
    "for name, group in dfg:\n",
    "    #print(name)\n",
    "    if i == 0: \n",
    "        firstValue = group['PortfolioValue'].iloc[0]\n",
    "        i = 1\n",
    "    else:\n",
    "        firstValue = lastValue\n",
    "\n",
    "    lastValue = group['PortfolioValue'].loc[group.index[-1]]\n",
    "    change = lastValue/firstValue - 1\n",
    "    df.loc[group.index, 'Change_%_Yearly'] = change * 100\n",
    "    \n",
    "df\n",
    "\n",
    "df1 = df[['Year', 'Month', 'Change_%_Monthly', 'Change_%_Yearly']]\n",
    "df1 = df1.drop_duplicates()\n",
    "df1\n",
    "\n",
    "pivotTable = df1.pivot_table(values ='Change_%_Monthly', index =['Year', 'Change_%_Yearly'],\n",
    "                         columns =['Month'])\n",
    "pivotTable.columns = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN',\n",
    "                      'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "\n",
    "pivotTable.index\n",
    "\n",
    "def _color_red_or_green(val):\n",
    "    color = '#EE0000' if val < 0 else '#00EE00'\n",
    "    return 'background-color: %s' % color\n",
    "pivotTable=pivotTable.style.applymap(_color_red_or_green)\n",
    "display(pivotTable)\n",
    "\n",
    "import pandas as pd\n",
    "import openpyxl as op\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from openpyxl import Workbook, worksheet, load_workbook\n",
    "#pivotTable.to_excel(output_path+'/t1.xlsx')#.to_csv(r'C:\\Aman\\Nifty correlation\\nifty 50\\19-09\\pl 50 table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f920b53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
