{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f0cd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime as dt\n",
    "from datetime import datetime, date, time,timedelta\n",
    "import pandas as pd\n",
    "from csv import DictWriter\n",
    "import urllib\n",
    "import re,datetime\n",
    "import os\n",
    "import time\n",
    "from os import walk\n",
    "import re\n",
    "import gc\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 500)\n",
    "\n",
    "\n",
    "#############\n",
    "#INPUTS\n",
    "#############\n",
    "\n",
    "delta = 15\n",
    "\n",
    "index = \"BANKNIFTY\"\n",
    "index_file_path = \"C:/Users/admin/Desktop/Dirty COrrelation BAcktest/All Data/Index/BANKNIFTY.csv\"\n",
    "\n",
    "stfolder = r\"C:/Users/admin/Desktop/Dirty COrrelation BAcktest/All Data/Stocks\"\n",
    "    \n",
    "wgt_lot_path = \"C:/Users/admin/Desktop/Dirty COrrelation BAcktest/All Data/Weights/BN Mcap & includeok - Copy.csv\"\n",
    "\n",
    "direc = r\"C:/Users/admin/Desktop/Dirty COrrelation BAcktest/Output Files/Only Correlation - total premium adjst - stock & index - Delta_\" + str(delta)\n",
    "\n",
    "output_path = r\"C:/Users/admin/Desktop/Dirty COrrelation BAcktest/Working Files/Only Correlation - total premium adjst - stock & index - Delta_\" + str(delta)\n",
    "\n",
    "index_lotsize = 1\n",
    "\n",
    "initial_equity = 100000\n",
    "\n",
    "\n",
    "#########################\n",
    "#Creating Stock List\n",
    "#########################\n",
    "\n",
    "filename = next(walk(stfolder), (None, None, []))[2]  # [] if no file\n",
    "stock_list_path = []\n",
    "for i in filename:\n",
    "    temp = stfolder +\"/\"+ i\n",
    "    stock_list_path.append(temp)\n",
    "\n",
    "    \n",
    "stock_list = []\n",
    "for i in stock_list_path:\n",
    "    stock_list.append(i.replace(\".csv\",\"\").replace(stfolder + \"/\",\"\"))\n",
    "print(stock_list)\n",
    "    \n",
    "######################################    \n",
    "#Creating weight and lot dictionary\n",
    "######################################\n",
    "\n",
    "\"\"\"wgtlot_df = pd.read_csv(wgt_lot_path)\n",
    "wgtlot_dict = wgtlot_df.set_index('Symbol').T.to_dict('list')\n",
    "print(wgtlot_dict)\n",
    "symbols = list(wgtlot_dict.keys())\"\"\"\n",
    "\n",
    "### Now adding Columns for weights below\n",
    "\n",
    "######################################\n",
    "\n",
    "lookback_period = 15\n",
    "std_dev = 2\n",
    "\n",
    "####################\n",
    "#EXPIRY DATES\n",
    "####################\n",
    "symbols = stock_list.copy()\n",
    "symbols.append(index)\n",
    "\n",
    "exp_file_path = \"expiry_dates.csv\"\n",
    "exp_df = pd.read_csv(exp_file_path,parse_dates = [\"curr_exp_date\",\"Date\"],dayfirst =True,usecols = [\"curr_exp_date\",\"Date\"])\n",
    "#exp_df.rename({'curr_date': 'Date'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53704acd",
   "metadata": {},
   "source": [
    "### Separating Current Month, Next Month and Far Month data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5351d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.read_csv(index_file_path,parse_dates = [\"Date\"],dayfirst = True)\n",
    "df1 = idx[idx['Ticker'].str.endswith('- I')]\n",
    "df2 = idx[idx['Ticker'].str.endswith('- II')]\n",
    "df3 = idx[idx['Ticker'].str.endswith('- III')]\n",
    "\n",
    "\n",
    "\n",
    "index_file_path = direc + \"/BANKNIFTY.csv\"\n",
    "\n",
    "df1.to_csv(index_file_path)\n",
    "df2.to_csv(index_file_path.replace(\"BANKNIFTY\",\"BANKNIFTY-II\"))\n",
    "df3.to_csv(index_file_path.replace(\"BANKNIFTY\",\"BANKNIFTY-III\"))\n",
    "\n",
    "print(idx.shape[0])\n",
    "print(df1.shape[0]+df2.shape[0]+df3.shape[0])\n",
    "    \n",
    "for i in stock_list:\n",
    "    df = pd.read_csv(stfolder + '/' + i + \".csv\",parse_dates = [\"Date\"],dayfirst = True)\n",
    "    df1 = df[df['Ticker'].str.endswith('- I')]\n",
    "    df2 = df[df['Ticker'].str.endswith('- II')]\n",
    "    df3 = df[df['Ticker'].str.endswith('- III')]\n",
    "    \n",
    "    path = direc + '/' + i + \".csv\"\n",
    "    \n",
    "    df1.to_csv(path)\n",
    "    df2.to_csv(path.replace(i,i+\"-II\"))\n",
    "    df3.to_csv(path.replace(i,i+\"-III\"))\n",
    "    \n",
    "    print(df.shape[0])\n",
    "    print(df1.shape[0]+df2.shape[0]+df3.shape[0])\n",
    "\n",
    "stfolder = direc\n",
    "print(index_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990df0a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "idx = pd.read_csv(index_file_path,usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                    \"OPT_Close\",\"EQ_Close\", \"IV\", f'{otm}_%_OTM_Strike',parse_dates = [\"Date\"])\n",
    "\n",
    "idx = pd.merge(idx, exp_df, on = 'Date', how = 'left')\n",
    "\n",
    "idx_ce = idx[(idx[\"Strike\"] == idx[f'{otm}_%_OTM_Strike']) & (idx[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "idx_pe = idx[(idx[\"Strike\"] == idx[f'{otm}_%_OTM_Strike']) & (idx[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "\n",
    "\n",
    "final = idx_ce.merge(idx_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "\n",
    "final = final.T.drop_duplicates().T  #Drop Duplicates Columns\n",
    "\n",
    "final = final.add_prefix(index +\"_\")\n",
    "final = final.sort_values(by=index +\"_\"+'Date_CE')\n",
    "\n",
    "final.to_csv(output_path+\"/check-1data.csv\")\n",
    "\n",
    "for i in stock_list:\n",
    "    print(i)\n",
    "    df1 = pd.read_csv(stfolder+ '/' + i + \".csv\",parse_dates = [\"Date\"],usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                    \"OPT_Close\",\"EQ_Close\",\"IV\"f'{otm}_%_OTM_Strike'])\n",
    "\n",
    "    df1 = pd.merge(df1, exp_df, on = 'Date', how = 'left')\n",
    "    \n",
    "    \n",
    "    df_ce = df1[(df1[\"Strike\"] == df1[f'{otm}_%_OTM_Strike']) & (df1[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "    df_pe = df1[(df1[\"Strike\"] == df1[f'{otm}_%_OTM_Strike']) & (df1[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "    print(df1.shape[0], df_ce.shape[0], df_pe.shape[0])\n",
    "    temp = df_ce.merge(df_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "    temp = temp.T.drop_duplicates().T\n",
    "    temp = temp.add_prefix(i +\"_\")\n",
    "    \n",
    "    final = final.merge(temp,how='left',left_on=index + '_Date_CE', right_on= i + '_Date_CE')\n",
    "    \n",
    "final = final.T.drop_duplicates().T\n",
    "curr_final_df = final.copy()\n",
    "u = curr_final_df.select_dtypes(exclude=['datetime'])\n",
    "curr_final_df[u.columns] = u.fillna(0)\n",
    "\n",
    "curr_final_df.to_csv(output_path+\"/check-2data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36939c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.read_csv(index_file_path.replace(\"BANKNIFTY\",\"BANKNIFTY-II\"),usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                    \"OPT_Close\",\"EQ_Close\",\"IV\"f'{otm}_%_OTM_Strike'],parse_dates = [\"Date\"])\n",
    "\n",
    "\n",
    "idx = pd.merge(idx, exp_df, on = 'Date', how = 'left')\n",
    "\n",
    "\n",
    "idx_ce = idx[(idx[\"Strike\"] == idx[f'{otm}_%_OTM_Strike']) & (idx[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "idx_pe = idx[(idx[\"Strike\"] == idx[f'{otm}_%_OTM_Strike') & (idx[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "\n",
    "\n",
    "final = idx_ce.merge(idx_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "\n",
    "final = final.T.drop_duplicates().T  #Drop Duplicates Columns\n",
    "\n",
    "final = final.add_prefix(index +\"_\")\n",
    "final = final.sort_values(by=index +\"_\"+'Date_CE')\n",
    "final.to_csv(output_path+\"/check-3data.csv\")\n",
    "\n",
    "for i in stock_list:\n",
    "    print(i)\n",
    "    path  = stfolder+ '/' + i + \".csv\"\n",
    "    df1 = pd.read_csv(path.replace(i,i+\"-II\"),parse_dates = [\"Date\"],usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                    \"OPT_Close\",\"EQ_Close\",\"IV\"f'{otm}_%_OTM_Strike'])\n",
    "    df1 = pd.merge(df1, exp_df, on = 'Date', how = 'left')\n",
    "\n",
    "    \n",
    "    df_ce = df1[(df1[\"Strike\"] == df1[f'{otm}_%_OTM_Strike']) & (df1[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "    df_pe = df1[(df1[\"Strike\"] == df1[f'{otm}_%_OTM_Strike']) & (df1[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "    \n",
    "    \n",
    "    temp = df_ce.merge(df_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "    temp = temp.T.drop_duplicates().T\n",
    "    temp = temp.add_prefix(i +\"_\")\n",
    "    final = final.merge(temp,how='left',left_on=index + '_Date_CE', right_on= i + '_Date_CE')\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "final = final.T.drop_duplicates().T\n",
    "next_final_df = final.copy()\n",
    "\n",
    "next_final_df.to_csv(output_path+\"/check-4.0data.csv\")\n",
    "\n",
    "u = next_final_df.select_dtypes(exclude=['datetime'])\n",
    "next_final_df[u.columns] = u.fillna(0)\n",
    "\n",
    "next_final_df.to_csv(output_path+\"/check-4data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6506d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacedf = curr_final_df[curr_final_df[index + \"_Date_CE\"] == curr_final_df[index + \"_curr_exp_date_CE\"]]\n",
    "restdf = curr_final_df[~(curr_final_df[index + \"_Date_CE\"] == curr_final_df[index + \"_curr_exp_date_CE\"])]\n",
    "\n",
    "nextdf = next_final_df[next_final_df[index + \"_Date_CE\"] == next_final_df[index + \"_curr_exp_date_CE\"]]\n",
    "\n",
    "\n",
    "for i in symbols:\n",
    "    if nextdf.shape[0] == replacedf.shape[0]:\n",
    "        replacedf[i + \"_IV_CE\"] = nextdf[i + \"_IV_CE\"]\n",
    "        replacedf[i + \"_IV_PE\"] = nextdf[i + \"_IV_PE\"]\n",
    "        replacedf[i + '_Strike_CE'] = nextdf[i + '_Strike_CE']\n",
    "        replacedf[i + '_Strike_PE'] = nextdf[i + '_Strike_PE']\n",
    "\n",
    "        \n",
    "        print(\"EXCHANGE DONE\")\n",
    "    else:\n",
    "        print(\"Entries Missing Error\")\n",
    "        break\n",
    "\n",
    "final = restdf.append(replacedf).sort_values(by=[index+\"_Date_CE\"]).reset_index().drop(columns = [\"index\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(output_path+\"\\Part-1data.csv\")\n",
    "data = pd.read_csv(output_path+\"\\Part-1data.csv\",parse_dates = [index + \"_Date_CE\"],dayfirst = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b1a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Avg IV Columns\n",
    "data[index + \"_Avg IV\"] = (data[index + \"_IV_CE\"] + data[index + \"_IV_PE\"])/2\n",
    "curr_final_df[index + \"_Avg IV\"] = data[index + \"_Avg IV\"]\n",
    "\n",
    "\n",
    "for i in stock_list:\n",
    "    data[i + \"_Avg IV\"] = (data[i + \"_IV_CE\"] + data[i + \"_IV_PE\"])/2\n",
    "    curr_final_df[i + \"_Avg IV\"] = data[i + \"_Avg IV\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "data = curr_final_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc6d08d",
   "metadata": {},
   "source": [
    "# Adding Weights Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbf9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = pd.read_csv(wgt_lot_path,parse_dates = [\"date\"],dayfirst = True,usecols = [\"Security Symbol\",\"Weightage\",\"date\",\"includeok\",\"Lotsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfc0d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grp = weight.groupby(\"Security Symbol\")\n",
    "data[index + \"_OG_Weight\"] = 100\n",
    "for i in stock_list:\n",
    "    d1 = grp.get_group(i)\n",
    "    d1[i + \"_OG_Weight\"] = d1[\"Weightage\"]\n",
    "    d1[index+ \"_Date_CE\"] = d1[\"date\"]\n",
    "    data = pd.merge(data,d1[[index+ \"_Date_CE\",i + \"_OG_Weight\"]],on= index+ \"_Date_CE\", how='left')\n",
    "\n",
    "if data.isnull().sum().sum() != 0:\n",
    "    print(data.isnull().sum().sum())\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5551d428",
   "metadata": {},
   "source": [
    "### Adding Final Weight Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81750a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in data if col.endswith('Weight')]\n",
    "filter_col.remove(index + \"_OG_Weight\")\n",
    "\n",
    "data[\"actual_wgt_sum\"] = data[filter_col].sum(axis=1)\n",
    "\n",
    "for i in stock_list:\n",
    "    data[i + \"_Final_Weight\"] = data[i + \"_OG_Weight\"]/data[\"actual_wgt_sum\"]\n",
    "data[index + \"_Final_Weight\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7666e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "u = data.select_dtypes(exclude=['datetime'])\n",
    "data[u.columns] = u.fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30727f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['index_iv'] = data[index +'_Avg IV']*100\n",
    "data['stock_iv']  = 0\n",
    "\n",
    "\n",
    "for i in stock_list:\n",
    "    data['stock_iv'] += data[i + '_Avg IV']*data[i + \"_Final_Weight\"] \n",
    "    \n",
    "    \n",
    "data['implied_correl'] = (data['index_iv']/data['stock_iv'])\n",
    "\n",
    "print(data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f458a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['avg'] = data['implied_correl'].rolling(lookback_period).mean()\n",
    "df = data.copy()\n",
    "\n",
    "df.to_csv(output_path+'/impliedratio.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sma(prices, rate):\n",
    "    return prices.rolling(rate).mean()\n",
    "\n",
    "def get_bollinger_bands(prices, rate):\n",
    "    sma = get_sma(prices, rate)\n",
    "    std = prices.rolling(rate).std()\n",
    "    bollinger_up = sma + std * std_dev # Calculate top band\n",
    "    bollinger_down = sma - std * std_dev # Calculate bottom band\n",
    "    return bollinger_up, bollinger_down\n",
    "\n",
    "df['bollinger_up'], df['bollinger_down'] = get_bollinger_bands(df['implied_correl'], lookback_period)\n",
    "\n",
    "df['sma'] = get_sma(df['implied_correl'], lookback_period)\n",
    "df['long_avg'] = df['implied_correl'].mean()\n",
    "\n",
    "\n",
    "#print(df['bollinger_up'])\n",
    "#print(df['bollinger_down'])\n",
    "#print(df['long_avg'])\n",
    "#print(df['sma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0589e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df.head()\n",
    "\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "plt.title(' Bollinger Bands')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Correl')\n",
    "plt.plot(df['implied_correl'], label='Correl', c = 'black')\n",
    "plt.plot(df['bollinger_up'], label='Bollinger Up', c='g')\n",
    "plt.plot(df['bollinger_down'], label='Bollinger Down', c='r')\n",
    "plt.plot(df['sma'], label='sma', c='b')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f75918f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(output_path+\"\\series.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727658d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def implement_bb_strategy(data, lower_bb, upper_bb, avg):\n",
    "    entry_ratio = []\n",
    "    exit_ratio = []\n",
    "    bb_signal = []\n",
    "    signal = 0\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        if(i==0):\n",
    "            if data[i] < lower_bb[i]:\n",
    "                if signal != 1:\n",
    "                    entry_ratio.append(data[i])\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    signal = 1\n",
    "                    bb_signal.append(signal)\n",
    "                else:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    bb_signal.append(0)\n",
    "            elif data[i] > upper_bb[i]:\n",
    "                if signal != -1:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(data[i])\n",
    "                    signal = -1\n",
    "                    bb_signal.append(signal)\n",
    "                else:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    bb_signal.append(0)\n",
    "            else:\n",
    "                entry_ratio.append(np.nan)\n",
    "                exit_ratio.append(np.nan)\n",
    "                bb_signal.append(0)\n",
    "        else:\n",
    "            if data[i-1] > lower_bb[i-1] and data[i] < lower_bb[i]:\n",
    "                if signal != 1:\n",
    "                    entry_ratio.append(data[i])\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    signal = 1\n",
    "                    bb_signal.append(signal)\n",
    "                else:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    bb_signal.append(bb_signal[i-1])\n",
    "\n",
    "            elif data[i-1] > avg[i-1] and data[i] < avg[i]:\n",
    "                if signal != 0:\n",
    "                    entry_ratio.append(data[i])\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    signal = 0\n",
    "                    bb_signal.append(signal)\n",
    "                else:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    bb_signal.append(bb_signal[i-1])                \n",
    "            \n",
    "            elif data[i-1] < upper_bb[i-1] and data[i] > upper_bb[i]:\n",
    "                if signal != -1:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(data[i])\n",
    "                    signal = -1\n",
    "                    bb_signal.append(signal)\n",
    "                else:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    bb_signal.append(bb_signal[i-1])\n",
    "\n",
    "            elif data[i-1] < avg[i-1] and data[i] > avg[i]:\n",
    "                if signal != 0:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(data[i])\n",
    "                    signal = 0\n",
    "                    bb_signal.append(signal)\n",
    "                else:\n",
    "                    entry_ratio.append(np.nan)\n",
    "                    exit_ratio.append(np.nan)\n",
    "                    bb_signal.append(bb_signal[i-1])\n",
    "\n",
    "            else:\n",
    "                entry_ratio.append(np.nan)\n",
    "                exit_ratio.append(np.nan)\n",
    "                bb_signal.append(bb_signal[i-1])\n",
    "\n",
    "    return entry_ratio, exit_ratio, bb_signal\n",
    "\n",
    "df['entry_ratio'], df['exit_ratio'], df['bb_signal'] = implement_bb_strategy(df['implied_correl'], df['bollinger_down'], df['bollinger_up'], df['avg'])\n",
    "\n",
    "fig = plt.figure(figsize=(14,7))\n",
    "\n",
    "\n",
    "df['implied_correl'].plot(label = 'Correlation', alpha = 1)\n",
    "df['bollinger_up'].plot(label = 'UPPER BB', linestyle = '--', linewidth = 1, color = 'black')\n",
    "df['avg'].plot(label = 'avg', linestyle = '--', linewidth = 1.2, color = 'grey')\n",
    "df['bollinger_down'].plot(label = 'LOWER BB', linestyle = '--', linewidth = 1, color = 'black')\n",
    "plt.scatter(df.index, df['entry_ratio'], marker = '^', color = 'green', label = 'BUY', s = 150)\n",
    "plt.scatter(df.index, df['exit_ratio'], marker = 'v', color = 'red', label = 'SELL', s = 150)\n",
    "plt.title('Bollinger Band')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0e595",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c961bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['dispersion_entry'] = False #(df1['bb_signal'] == -1) & (df1['exit_ratio'] > 0)  & (df1['entry_ratio'].isna() == True)\n",
    "df1['dispersion_exit'] = False #(df1['bb_signal'] == 0) & (df1['entry_ratio'] > 0) & (df1['entry_ratio'].isna() == False)\n",
    "\n",
    "df1['correlation_entry'] = np.nan\n",
    "df1['correlation_exit'] = np.nan\n",
    "df1['bb_signal'] = np.nan\n",
    "\n",
    "#df1['correlation_entry'] = (df1['bb_signal'] == 1) & (df1['entry_ratio'] > 0)  & (df1['exit_ratio'].isna() == True)\n",
    "#df1['correlation_exit'] = (df1['bb_signal'] == 0)  & (df1['exit_ratio'] > 0)  & (df1['exit_ratio'].isna() == False):\n",
    "\n",
    "df1['bb_signal'] = df1.apply(lambda x: 0 if x['BANKNIFTY_Date_CE']==x['BANKNIFTY_curr_exp_date_CE'] else 1, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(df1)):\n",
    "    if(i==0):\n",
    "        df1.loc[i,'correlation_entry'] = True\n",
    "        df1.loc[i,'correlation_exit'] = False\n",
    "    elif (df1.loc[i-1, 'BANKNIFTY_Date_CE'] == df1.loc[i-1, 'BANKNIFTY_curr_exp_date_CE']):  \n",
    "        df1.loc[i,'correlation_entry'] = True\n",
    "        df1.loc[i,'correlation_exit'] = False\n",
    "    elif (df1.loc[i,'BANKNIFTY_Date_CE'] == df1.loc[i,'BANKNIFTY_curr_exp_date_CE']):\n",
    "        df1.loc[i,'correlation_entry'] = False\n",
    "        df1.loc[i,'correlation_exit'] = True\n",
    "    else:\n",
    "        df1.loc[i,'correlation_entry'] = False\n",
    "        df1.loc[i,'correlation_exit'] = False\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d448cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "\n",
    "\n",
    "df2.to_csv(output_path + '//' + 'with entry exit signals_all data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872aaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all the rows with entry or exit signals\n",
    "df = df2[ (df2[\"dispersion_entry\"] == True) | (df2[\"correlation_exit\"] == True) | \n",
    "              (df2[\"dispersion_exit\"] == True) | (df2[\"correlation_entry\"] == True) ].reset_index()\n",
    "\n",
    "df.to_csv(output_path + '//' + 'with entry exit signals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3107dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades = []\n",
    "\n",
    "for i in df.index[:-1]:\n",
    "    if df[\"bb_signal\"][i] == -1:\n",
    "        strikes = {}\n",
    "        strikes[index + '_Strike_CE'] = df[index + \"_Strike_CE\"][i]\n",
    "        strikes[index + '_Strike_PE'] = df[index + \"_Strike_PE\"][i]\n",
    "            \n",
    "        for j in stock_list:\n",
    "            strikes[j + '_Strike_CE'] = df[j +\"_Strike_CE\"][i]\n",
    "            strikes[j + '_Strike_PE'] = df[j + \"_Strike_PE\"][i]\n",
    "            \n",
    "        trades.append( \n",
    "                        { \n",
    "                          \"entry_date\": df[index + \"_Date_CE\"][i],\n",
    "                          \"exit_date\" : df[index + \"_Date_CE\"][i+1],\n",
    "                          \"expiry_date\" : df[index+\"_curr_exp_date_CE\"][i],\n",
    "                          \"type_of_trade\": \"SILS\",\n",
    "                          \"strike\" : strikes,\n",
    "                          \"entry_ratio\" : df[\"exit_ratio\"][i],\n",
    "                          \"exit_ratio\" : df[\"entry_ratio\"][i+1]\n",
    "                        }\n",
    "                      )\n",
    "                            \n",
    "    if df[\"bb_signal\"][i] == 1:\n",
    "            strikes = {}\n",
    "            strikes[index + '_Strike_CE'] = df[index + \"_Strike_CE\"][i]\n",
    "            strikes[index + '_Strike_PE'] = df[index + \"_Strike_PE\"][i]\n",
    "\n",
    "            for j in stock_list:\n",
    "                strikes[j + '_Strike_CE'] = df[j + \"_Strike_CE\"][i]\n",
    "                strikes[j + '_Strike_PE'] = df[j + \"_Strike_PE\"][i]\n",
    "\n",
    "            trades.append( \n",
    "                            { \n",
    "                              \"entry_date\": df[index + \"_Date_CE\"][i],\n",
    "                              \"exit_date\" : df[index + \"_Date_CE\"][i+1],\n",
    "                              \"expiry_date\" : df[index+\"_curr_exp_date_CE\"][i],\n",
    "                              \"type_of_trade\": \"LISS\",\n",
    "                              \"strike\" : strikes,\n",
    "                              \"entry_ratio\" : df[\"entry_ratio\"][i],\n",
    "                              \"exit_ratio\" : df[\"exit_ratio\"][i+1]\n",
    "                            }\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac20e1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in tqdm(trades):\n",
    "    date1 = i[\"entry_date\"]\n",
    "    date2 = i[\"exit_date\"]\n",
    "    expiry = i[\"expiry_date\"]\n",
    "    print(date1)\n",
    "    print(date2)\n",
    "    \n",
    "    if expiry == date1:\n",
    "        df = pd.read_csv(index_file_path,parse_dates = [\"Date\"])\n",
    "        nextdf = pd.read_csv(index_file_path.replace(index,index+\"-II\"),parse_dates = [\"Date\"])\n",
    "        \n",
    "        df = df[(df[\"Date\"] > date1) & (df[\"Date\"] <= date2) &\n",
    "                (((df[\"Strike\"] == i[\"strike\"][index + '_Strike_CE']) & (df[\"Option_Type\"] == 'CE')) |\n",
    "                 ((df[\"Strike\"] == i[\"strike\"][index + '_Strike_PE']) & (df[\"Option_Type\"] == 'PE')))]\n",
    "        nextdf = nextdf[(nextdf[\"Date\"] == expiry) & \n",
    "                        (((nextdf[\"Strike\"] == i[\"strike\"][index + '_Strike_CE']) & (nextdf['Option_Type'] == 'CE')) |\n",
    "                        ((nextdf['Strike'] == i['strike'][index + '_Strike_PE']) & (nextdf['Option_Type'] == 'PE')))]\n",
    "        \n",
    "        nextdf = nextdf.append(df)\n",
    "        nextdf = pd.merge(nextdf, exp_df, on = 'Date', how = 'left')\n",
    "        df_dict = {}\n",
    "        df_dict[index] = nextdf\n",
    "        for j in stock_list:\n",
    "            path = stfolder + '/' + j + \".csv\"\n",
    "            df = pd.read_csv(path,parse_dates = [\"Date\"])\n",
    "            nextdf = pd.read_csv(path.replace(j,j+\"-II\"),parse_dates = [\"Date\"])\n",
    "                        \n",
    "            df = df[(df[\"Date\"] > date1) & (df[\"Date\"] <= date2) &\n",
    "                (((df[\"Strike\"] == i[\"strike\"][j + '_Strike_CE']) & (df[\"Option_Type\"] == 'CE')) |\n",
    "                 ((df[\"Strike\"] == i[\"strike\"][j + '_Strike_PE']) & (df[\"Option_Type\"] == 'PE')))]   \n",
    "            \n",
    "            \n",
    "            nextdf = nextdf[(nextdf[\"Date\"] == expiry) & \n",
    "                        (((nextdf[\"Strike\"] == i[\"strike\"][index + '_Strike_CE']) & (nextdf['Option_Type'] == 'CE')) |\n",
    "                        ((nextdf['Strike'] == i['strike'][index + '_Strike_PE']) & (nextdf['Option_Type'] == 'PE')))]\n",
    "            nextdf = nextdf.append(df)\n",
    "            nextdf = pd.merge(nextdf, exp_df, on = 'Date', how = 'left')\n",
    "            df_dict[j] = nextdf\n",
    "        i[\"dataframes\"] = df_dict\n",
    "    \n",
    "    else:\n",
    "        df = pd.read_csv(index_file_path,parse_dates = [\"Date\"])\n",
    "        df = df[(df[\"Date\"] >= date1) & (df[\"Date\"] <= date2) &\n",
    "                (((df[\"Strike\"] == i[\"strike\"][index + '_Strike_CE']) & (df[\"Option_Type\"] == 'CE')) |\n",
    "                 ((df[\"Strike\"] == i[\"strike\"][index + '_Strike_PE']) & (df[\"Option_Type\"] == 'PE')))]\n",
    "        df = pd.merge(df, exp_df, on = 'Date', how = 'left')\n",
    "        df_dict = {}\n",
    "        df_dict[index] = df\n",
    "        \n",
    "        for j in stock_list:\n",
    "            df = pd.read_csv(stfolder + '/' + j + \".csv\",parse_dates = [\"Date\"])    \n",
    "            df = df[(df[\"Date\"] >= date1) & (df[\"Date\"] <= date2) &\n",
    "                (((df[\"Strike\"] == i[\"strike\"][j + '_Strike_CE']) & (df[\"Option_Type\"] == 'CE')) |\n",
    "                 ((df[\"Strike\"] == i[\"strike\"][j + '_Strike_PE']) & (df[\"Option_Type\"] == 'PE')))]\n",
    "                \n",
    "            df = pd.merge(df, exp_df, on = 'Date', how = 'left')\n",
    "            df_dict[j] = df\n",
    "        i[\"dataframes\"] = df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870c44a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# check what does trades[i]['dataframes'] contains\n",
    "for i in trades:\n",
    "    print(i[\"dataframes\"]['BANKNIFTY'].columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ffb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame()\n",
    "for trade_dict in trades:\n",
    "    \n",
    "    # df_dict contains options details for all the stocks and index for all the dates between entry and exit date\n",
    "    df_dict = trade_dict[\"dataframes\"]\n",
    "    entry = trade_dict[\"entry_date\"]\n",
    "    exit = trade_dict[\"exit_date\"]\n",
    "    \n",
    "    idx = df_dict[index][[\"Date\",\"Ticker\",\"OPT_Close\",\"Strike\",\"Option_Type\",\"IV\", 'Delta',\"EQ_Close\",\"curr_exp_date\"]]\n",
    "    \n",
    "    idx_ce = idx[(idx[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "    idx_pe = idx[(idx[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    final = idx_ce.merge(idx_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "    final = final.add_prefix(index +\"_\")\n",
    "\n",
    "    for i in df_dict:\n",
    "        if i == index:\n",
    "            continue\n",
    "        df1 = df_dict[i][[\"Date\",\"Ticker\",\"OPT_Close\",\"Strike\",\"Option_Type\",\"IV\", 'Delta', \"EQ_Close\",\"curr_exp_date\"]]\n",
    "        df_ce = df1[(df1[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\")\n",
    "        df_pe = df1[(df1[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\")\n",
    "\n",
    "        temp = df_ce.merge(df_pe, left_on='Date_CE', right_on='Date_PE').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "        temp = temp.T.drop_duplicates().T\n",
    "        temp = temp.add_prefix(i +\"_\")\n",
    "        \n",
    "        final = final.merge(temp,left_on=index + '_Date_CE' , right_on= i + '_Date_CE', how = 'left')\n",
    "    final[\"Trade_Type\"] = trade_dict[\"type_of_trade\"]\n",
    "    \n",
    "    final.to_csv(output_path + '//' + 'final.csv', index=False)\n",
    "\n",
    "    conditions = [\n",
    "    (final[index + '_Date_CE'] == entry),\n",
    "    (final[index + '_Date_CE'] == exit),\n",
    "    (final[index + '_Date_CE'] != exit) & (final[index + '_Date_CE'] != entry)]\n",
    "\n",
    "    # create a list of the values we want to assign for each condition\n",
    "    values = ['F', 'L', 'M']\n",
    "\n",
    "    # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "    final['D_marker'] = np.select(conditions, values)\n",
    "    result = result.append(final)\n",
    "        \n",
    "result = result.T.drop_duplicates().T\n",
    "result.to_csv(output_path + '//' + 'final1.csv', index=False)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f48670",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = result.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7789d522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(data)\n",
    "data.to_csv(output_path + '//' + 'before_weigt_add_second_time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ea0a8f",
   "metadata": {},
   "source": [
    "### Adding Weight and Lot Columns Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18afe0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = pd.read_csv(wgt_lot_path,parse_dates = [\"date\"],dayfirst = True,usecols = [\"Security Symbol\",\"Weightage\",\"date\",\"Lotsize\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b62b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "grp = weight.groupby(\"Security Symbol\")\n",
    "data[index + \"_OG_Weight\"] = 100\n",
    "data[index + \"_Lotsize\"] = index_lotsize\n",
    "for i in stock_list:\n",
    "    d1 = grp.get_group(i)\n",
    "    d1[i + \"_OG_Weight\"] = d1[\"Weightage\"]\n",
    "    d1[i + \"_Lotsize\"] = d1[\"Lotsize\"]\n",
    "    d1[index+ \"_Date_CE\"] = d1[\"date\"]\n",
    "    data = pd.merge(data,d1[[index+ \"_Date_CE\",i + \"_OG_Weight\",i+ \"_Lotsize\"]],on= index+ \"_Date_CE\", how='left')\n",
    "\n",
    "if data.isnull().sum().sum() != 0:\n",
    "    print(data.isnull().sum().sum())\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "    print(\"NULL VALUES PRESENT ERROR\")\n",
    "\n",
    "u = data.select_dtypes(exclude=['datetime'])\n",
    "data[u.columns] = u.fillna(0)\n",
    "\n",
    "\n",
    "data.to_csv(output_path + '//' + 'weigt_add_second_time.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d041c8",
   "metadata": {},
   "source": [
    "### Adding Final Weight Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23097ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in data if col.endswith('Weight')]\n",
    "filter_col.remove(index + \"_OG_Weight\")\n",
    "\n",
    "data[\"actual_wgt_sum\"] = data[filter_col].sum(axis=1)\n",
    "for i in stock_list:\n",
    "    data[i + \"_Final_Weight\"] = data[i + \"_OG_Weight\"]/data[\"actual_wgt_sum\"]\n",
    "data[index + \"_Final_Weight\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071b5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_col = [col for col in data if col.endswith('Final_Weight')]\n",
    "filter_col.remove(index + \"_Final_Weight\")\n",
    "\n",
    "data[filter_col] = data[filter_col].replace(0,np.nan)\n",
    "data[\"Min_Share_Value\"] = data[filter_col].min(axis = 1)\n",
    "data[\"Min_Share_Name\"] = data[filter_col].idxmin(axis=1)\n",
    "print(data[\"Min_Share_Value\"])\n",
    "print(data[\"Min_Share_Name\"])\n",
    "\n",
    "data[filter_col] = data[filter_col].replace(np.nan,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da097072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(data)\n",
    "data.to_csv(output_path + '//' + 'before_qty_add.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3072b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ADDING QTYS\n",
    "result = data.copy()\n",
    "min_exp = []\n",
    "for idi, row in result.iterrows():\n",
    "    min_share_stock = row['Min_Share_Name'].replace(\"_Final_Weight\",\"\")\n",
    "    #print(min_share_stock)\n",
    "    value  = row[min_share_stock + \"_EQ_Close_CE\"]*row[min_share_stock + \"_Lotsize\"]\n",
    "    min_exp.append(value)\n",
    "    \n",
    "result[\"MinExp\"] = min_exp\n",
    "result[index + \"_exp\"] = (result[\"MinExp\"]/result[\"Min_Share_Value\"])\n",
    "#result[index + \"_lot\"] = (result[index + \"_exp\"]/(result[index + \"_Lotsize\"]*result[index + \"_EQ_Close_CE\"])).astype(int)\n",
    "result[index + \"_lot\"] = (result[index + \"_exp\"]/(result[index + \"_Lotsize\"]*result[index + \"_EQ_Close_CE\"]))\n",
    "result[index + \"_qty\"] = result[index + \"_lot\"]*result[index + \"_Lotsize\"]\n",
    "\n",
    "result.to_csv(output_path + '//' + 'resultt.csv')\n",
    "\n",
    "#for stocks with includeok=0, making '_Lotsize' & '_EQ_Close_CE' non-zero\n",
    "\n",
    "filter_col_Lotsize = [col for col in result if col.endswith('_Lotsize')]\n",
    "filter_col_Lotsize.remove(index + \"_Lotsize\")\n",
    "result[filter_col_Lotsize] = result[filter_col_Lotsize].replace(0,1)\n",
    "\n",
    "\n",
    "filter_col_EQ_Close = [col for col in result if col.endswith('_EQ_Close_CE')]\n",
    "filter_col_EQ_Close.remove(index + \"_EQ_Close_CE\")\n",
    "result[filter_col_EQ_Close] = result[filter_col_EQ_Close].replace(0,1)\n",
    "\n",
    "result['index_exp'] = result[index + \"_exp\"]\n",
    "\n",
    "result.to_csv(output_path + '//' + 'resulttcheckcheck.csv')\n",
    "\n",
    "result['stock_exp'] = 0\n",
    "\n",
    "for i in stock_list:\n",
    "    print(i)\n",
    "    if(i==min_share_stock):\n",
    "        result[index + \"_exp\"]\n",
    "        result[i + \"_Final_Weight\"]\n",
    "        result[i + \"_exp\"] = result[\"MinExp\"]\n",
    "        result[i  + \"_lot\"] = 1\n",
    "        result[i + \"_qty\"] = result[i + \"_lot\"]*result[i + \"_Lotsize\"]\n",
    "        \n",
    "        result['stock_exp'] += result[i + \"_exp\"]\n",
    "        \n",
    "    else:\n",
    "        result[index + \"_exp\"]\n",
    "        result[i + \"_Final_Weight\"]\n",
    "        result[i + \"_exp\"] = (result[index + \"_exp\"]*result[i + \"_Final_Weight\"])\n",
    "#         result[i  + \"_lot\"] = (result[i + \"_exp\"]/(result[i + \"_Lotsize\"]*result[i+ \"_EQ_Close_CE\"])).astype(int)\n",
    "        result[i  + \"_lot\"] = (result[i + \"_exp\"]/(result[i + \"_Lotsize\"]*result[i+ \"_EQ_Close_CE\"]))\n",
    "        result[i + \"_qty\"] = result[i + \"_lot\"]*result[i + \"_Lotsize\"]\n",
    "        \n",
    "        result['stock_exp'] += result[i + \"_exp\"]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb88a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['total_exposure'] = (result['index_exp'] + result['stock_exp']) * 2\n",
    "\n",
    "conditions = [\n",
    "    (result[\"Trade_Type\"] == \"LISS\"),\n",
    "    (result[\"Trade_Type\"] == \"SILS\")]\n",
    "\n",
    "values = [result['total_exposure']/10,result['total_exposure']/10]\n",
    "result['margin'] = np.select(conditions, values)\n",
    "\n",
    "result['units'] = (initial_equity/result['margin'])\n",
    "\n",
    "result.to_csv(output_path+\"/final2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f8e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "    (result[\"D_marker\"] == \"F\"),\n",
    "    (result[\"D_marker\"] != \"F\")]\n",
    "\n",
    "values = [result[index + \"_qty\"]*result['units'],np.nan]\n",
    "result[index + '_qty_final'] = np.select(conditions, values)\n",
    "\n",
    "for i in stock_list:\n",
    "    values = [result[i + \"_qty\"]*result['units'],np.nan]\n",
    "    result[i + '_qty_final'] = np.select(conditions, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9203b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04ce6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(result)\n",
    "\n",
    "result.to_csv(output_path + \"/final3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3827855",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(curr_final_df)\n",
    "display(next_final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02935ea",
   "metadata": {},
   "source": [
    "# Adjustments Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529bdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv(output_path + \"/final3.csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "\n",
    "\n",
    "result1 = pd.read_csv(output_path + \"/final3.csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "\n",
    "result1[\"Date\"] = result1[\"BANKNIFTY_Date_CE\"]\n",
    "\n",
    "final = pd.DataFrame()\n",
    "final[index + '_Date_CE'] = result1[\"Date\"]\n",
    "for i in stock_list:\n",
    "    print(i)\n",
    "    path  = stfolder+ '/' + i + \".csv\"\n",
    "    df1 = pd.read_csv(path,parse_dates = [\"Date\"],usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                    \"OPT_Close\",f'{otm}_%_OTM_Strike','EQ_Close'], dayfirst = True)\n",
    "\n",
    "    df_ce = df1[(df1[\"Strike\"] == df1[f'{otm}_%_OTM_Strike']) & (df1[\"Option_Type\"] == \"CE\")].add_suffix(\"_CE\").add_prefix(\"Current_\")\n",
    "    df_pe = df1[(df1[\"Strike\"] == df1[f'{otm}_%_OTM_Strike']) & (df1[\"Option_Type\"] == \"PE\")].add_suffix(\"_PE\").add_prefix(\"Current_\")\n",
    "    \n",
    "    temp = df_ce.merge(df_pe, left_on='Current_Date_CE', right_on='Current_Date_PE').drop(columns = [\"Current_Option_Type_PE\",\"Current_Option_Type_CE\"])\n",
    "    temp = temp.T.drop_duplicates().T\n",
    "    temp = temp.add_prefix(i +\"_\")\n",
    "    final = final.merge(temp,how='left',left_on=index + '_Date_CE', right_on= i + '_Current_Date_CE').drop(columns = [i + \"_Current_Date_CE\"])\n",
    "final = final.T.drop_duplicates().T\n",
    "\n",
    "result1 = result1.merge(final, how = \"left\", on = index+'_Date_CE')\n",
    "result1.to_csv(output_path+\"/final3.1.csv\")\n",
    "display(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cd595b",
   "metadata": {},
   "source": [
    "# Trigger Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigger = [100, 125, 150, 175, 200]\n",
    "result = pd.read_csv(output_path + \"/final3.csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "\n",
    "\n",
    "\n",
    "for t in trigger:\n",
    "    print(\"trigger :\", t)\n",
    "\n",
    "    print(\"Generating trigger Signals.....\")\n",
    "    print(\"          \", datetime.datetime.now())\n",
    "    result1 = pd.read_csv(output_path + \"/final3.1.csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "\n",
    "    for idi, row in result1.iterrows():\n",
    "        if row[\"D_marker\"] == \"F\":\n",
    "\n",
    "            result1.loc[idi, index + 'total_premium'] = result1.loc[idi, index + '_OPT_Close_CE'] + result1.loc[idi, index + \"_OPT_Close_PE\"]\n",
    "\n",
    "            for i in stock_list:\n",
    "                result1.loc[idi, i + \"D_marker_1\"] = \"F\"\n",
    "                result1.loc[idi+1:, i + 'D_marker_1'] = \"M\"\n",
    "                result1.loc[idi, i + \"D_marker_2\"] = \"F\"\n",
    "                result1.loc[idi+1:, i + \"D_marker_2\"] = \"M\"\n",
    "\n",
    "\n",
    "                result1.loc[idi, i + 'total_premium'] = result1.loc[idi, i + '_Current_OPT_Close_CE'] + result1.loc[idi, i + \"_Current_OPT_Close_PE\"]\n",
    "                result1.loc[idi, i + 'stop_trigger_CE'] = t*(result1.loc[idi, i + '_Current_Strike_CE'] + result1.loc[idi, i + 'total_premium'])/100\n",
    "                result1.loc[idi, i + 'stop_trigger_PE'] = t*(result1.loc[idi, i + '_Current_Strike_PE'] - result1.loc[idi, i + 'total_premium'])/100\n",
    "                result1.loc[idi+1:, i + 'stop_trigger_CE'] = np.nan\n",
    "                result1.loc[idi+1:, i + 'stop_trigger_PE'] = np.nan\n",
    "\n",
    "            result1 = result1.ffill()\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "            for i in stock_list:\n",
    "\n",
    "                if result1.loc[idi, \"D_marker\"] == \"L\":\n",
    "                    result1.loc[idi, i + \"D_marker_1\"] = \"L\"\n",
    "                    result1.loc[idi, i + \"D_marker_2\"] = \"L\"\n",
    "\n",
    "                else:\n",
    "\n",
    "                    if result1.loc[idi, i + 'stop_trigger_CE'] <= result1.loc[idi, i + '_Current_EQ_Close_CE']:\n",
    "                        result1.loc[idi, i + \"D_marker_1\"] = \"L\"\n",
    "                        result1.loc[idi, i + \"D_marker_2\"] = \"F\"\n",
    "\n",
    "                        result1.loc[idi, i + 'total_premium'] = result1.loc[idi, i + '_Current_OPT_Close_CE'] + result1.loc[idi, i + \"_Current_OPT_Close_PE\"]\n",
    "                        result1.loc[idi, i + 'stop_trigger_CE'] = t*(result1.loc[idi, i + '_Current_Strike_CE'] + result1.loc[idi, i + 'total_premium'])/100\n",
    "                        result1.loc[idi, i + 'stop_trigger_PE'] = t*(result1.loc[idi, i + '_Current_Strike_PE'] - result1.loc[idi, i + 'total_premium'])/100\n",
    "                        result1.loc[idi+1:, i + 'stop_trigger_CE'] = np.nan\n",
    "                        result1.loc[idi+1:, i + 'stop_trigger_PE'] = np.nan\n",
    "\n",
    "\n",
    "                    if result1.loc[idi, i + 'stop_trigger_PE'] >= result1.loc[idi, i + '_Current_EQ_Close_CE']:\n",
    "                        result1.loc[idi, i + \"D_marker_1\"] = \"L\"\n",
    "                        result1.loc[idi, i + \"D_marker_2\"] = \"F\"\n",
    "\n",
    "                        result1.loc[idi, i + 'total_premium'] = result1.loc[idi, i + '_Current_OPT_Close_CE'] + result1.loc[idi, i + \"_Current_OPT_Close_PE\"]\n",
    "                        result1.loc[idi, i + 'stop_trigger_CE'] = t*(result1.loc[idi, i + '_Current_Strike_CE'] + result1.loc[idi, i + 'total_premium'])/100\n",
    "                        result1.loc[idi, i + 'stop_trigger_PE'] = t*(result1.loc[idi, i + '_Current_Strike_PE'] - result1.loc[idi, i + 'total_premium'])/100\n",
    "                        result1.loc[idi+1:, i + 'stop_trigger_CE'] = np.nan\n",
    "                        result1.loc[idi+1:, i + 'stop_trigger_PE'] = np.nan\n",
    "\n",
    "                    result1 = result1.ffill()\n",
    "\n",
    "\n",
    "    #display(result1)      \n",
    "\n",
    "    result1.to_csv(output_path + \"/final3.2__\" + str(t) + \".csv\")\n",
    "\n",
    "\n",
    "    #######################################################################################################################\n",
    "\n",
    "\n",
    "    print(\"Generating TradeList.....\")\n",
    "    print(\"          \", datetime.datetime.now())\n",
    "\n",
    "    trades = []\n",
    "    for j in stock_list:\n",
    "        df = result1[(result1[j + \"D_marker_1\"] == \"F\") | (result1[j + \"D_marker_1\"] == \"L\")].reset_index()\n",
    "        #display(df)\n",
    "\n",
    "        for i in df.index[:-1]:\n",
    "            if df[j + \"D_marker_2\"][i] == \"F\":\n",
    "                strikes_ce = {}\n",
    "                strikes_ce[j] = df[j+\"_Current_Strike_CE\"][i]\n",
    "                strikes_pe = {}\n",
    "                strikes_pe[j] = df[j+\"_Current_Strike_PE\"][i]\n",
    "                trades.append( \n",
    "                                { \n",
    "                                  \"symbol\" : j,\n",
    "                                  j + \"_entry_date\": df[index + \"_Date_CE\"][i],\n",
    "                                  j + \"_exit_date\" : df[index + \"_Date_CE\"][i+1],\n",
    "                                  \"expiry_date\" : df[index+\"_curr_exp_date_CE\"][i],\n",
    "                                  \"new_month_date\" : df[index+\"_curr_exp_date_CE\"][i+1],\n",
    "                                  \"strike_ce\" : strikes_ce,\n",
    "                                  \"strike_pe\" : strikes_pe,  \n",
    "                                  \"marker_1\" : df[j + \"D_marker_1\"][i],\n",
    "                                  \"marker_2\" : df[j + \"D_marker_2\"][i],\n",
    "\n",
    "                                }\n",
    "                              )\n",
    "\n",
    "\n",
    "    #######################################################################################################################\n",
    "\n",
    "\n",
    "    df_dum = pd.DataFrame()\n",
    "\n",
    "    df_dum['Date'] = result1['BANKNIFTY_Date_CE']\n",
    "\n",
    "    for j in stock_list:\n",
    "        print(j)\n",
    "        path = direc + '/' + j + \".csv\"\n",
    "        for i in trades:\n",
    "            symbol = i[\"symbol\"]\n",
    "            if j == symbol:\n",
    "                #print(\"####################\",j)\n",
    "                date1 = i[j + \"_entry_date\"]\n",
    "                date2 = i[j + \"_exit_date\"]\n",
    "                expiry = i[\"expiry_date\"]\n",
    "                new_month_date = i[\"new_month_date\"]\n",
    "                marker_1 = i[\"marker_1\"]\n",
    "                marker_2 = i[\"marker_2\"]\n",
    "    #             print(date1)\n",
    "    #             print(date2)\n",
    "    #             print(i[\"strike_ce\"][j])\n",
    "    #             print(i[\"strike_pe\"][j])\n",
    "\n",
    "                if ((marker_1 == \"F\") & (marker_2 == \"F\")):       #new trade data from same row\n",
    "                    df_dict = {}\n",
    "\n",
    "                    df = pd.read_csv(path,parse_dates = [\"Date\"], dayfirst = True, usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                        \"OPT_Close\",\"EQ_Close\",\"IV\"f'{otm}_%_OTM_Strike'])\n",
    "\n",
    "                    df_ce = df[(df[\"Date\"] >= date1) & (df[\"Date\"] <= date2) & (df[\"Option_Type\"] == \"CE\") & (df[\"Strike\"] == i[\"strike_ce\"][j])].add_suffix(\"_CE\")\n",
    "                    df_pe = df[(df[\"Date\"] >= date1) & (df[\"Date\"] <= date2) & (df[\"Option_Type\"] == \"PE\") & (df[\"Strike\"] == i[\"strike_pe\"][j])].add_suffix(\"_PE\")\n",
    "                    df = pd.merge(df_ce, df_pe, left_on = 'Date_CE',right_on = 'Date_PE', how = 'left').reset_index()\n",
    "                    df['Date'] = df['Date_CE']\n",
    "                    df = pd.merge(df_dum, df, on = 'Date', how = 'left').reset_index()\n",
    "                    df_dict[j] = df\n",
    "                    i[\"dataframes\"] = df_dict\n",
    "\n",
    "                elif (marker_1 == \"L\") & (marker_2 == \"F\"):     #new trade data from next row\n",
    "\n",
    "                    df_dict = {}\n",
    "\n",
    "                    df = pd.read_csv(path,parse_dates = [\"Date\"], dayfirst = True, usecols = [\"Date\",\"Ticker\",\"Option_Type\",\"Strike\",\n",
    "                                                                        \"OPT_Close\",\"EQ_Close\",\"IV\"f'{otm}_%_OTM_Strike'])\n",
    "\n",
    "                    df_ce = df[(df[\"Date\"] > date1) & (df[\"Date\"] <= date2) & (df[\"Option_Type\"] == \"CE\") & (df[\"Strike\"] == i[\"strike_ce\"][j])].add_suffix(\"_CE\")\n",
    "                    df_pe = df[(df[\"Date\"] > date1) & (df[\"Date\"] <= date2) & (df[\"Option_Type\"] == \"PE\") & (df[\"Strike\"] == i[\"strike_pe\"][j])].add_suffix(\"_PE\")\n",
    "                    df = pd.merge(df_ce, df_pe, left_on = 'Date_CE',right_on = 'Date_PE', how = 'left').reset_index()\n",
    "                    df['Date'] = df['Date_CE']\n",
    "                    df = pd.merge(df_dum, df, on = 'Date', how = 'left').reset_index()\n",
    "\n",
    "                    df_dict[j] = df\n",
    "                    i[\"dataframes\"] = df_dict\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    result1 = pd.DataFrame()\n",
    "    result1[index + \"_Date_CE\"] = result[index + \"_Date_CE\"]\n",
    "\n",
    "    for j in stock_list:\n",
    "        print(\"j\", j)\n",
    "        final = pd.DataFrame()\n",
    "\n",
    "        for trade_dict in trades:\n",
    "            df_dict = trade_dict[\"dataframes\"]\n",
    "            for i in df_dict:\n",
    "                    if (i != j):\n",
    "                        continue\n",
    "\n",
    "                    df_ce = df_dict[i][[\"Date_CE\",\"Ticker_CE\",\"OPT_Close_CE\",\"Strike_CE\",\"Option_Type_CE\",\"IV_CE\",\"EQ_Close_CE\"]]\n",
    "                    df_pe = df_dict[i][[\"Date_CE\",\"Ticker_PE\",\"OPT_Close_PE\",\"Strike_PE\",\"Option_Type_PE\",\"IV_PE\"]]\n",
    "                    df_ce = df_ce.dropna()\n",
    "                    df_pe = df_pe.dropna()\n",
    "\n",
    "                    temp = pd.merge(df_ce,df_pe, on='Date_CE', how='left').drop(columns = [\"Option_Type_PE\",\"Option_Type_CE\"])\n",
    "                    temp = temp.T.drop_duplicates().T\n",
    "                    temp = temp.add_prefix(i +\"_new_\")\n",
    "                    final = final.append(temp)\n",
    "\n",
    "        result1 = result1.merge(final,left_on=index + '_Date_CE' , right_on= j + '_new_Date_CE', how = 'left')\n",
    "\n",
    "    result1 = result1.T.drop_duplicates().T    \n",
    "    result1.to_csv(output_path + \"/final3.3__\" + str(t) + \".csv\")\n",
    "    #display(result1)    \n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    result1 = pd.read_csv(output_path + \"/final3.2__\" + str(t) + \".csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "    result2 = pd.read_csv(output_path + \"/final3.3__\" + str(t) + \".csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "\n",
    "\n",
    "    result = result1.merge(result2,how='left',on=index+'_Date_CE')\n",
    "\n",
    "    #result.to_csv(output_path + \"/final4.csv\")\n",
    "    #display(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################################################################\n",
    "    print(\"Calculating PnL.....\")\n",
    "    print(\"          \", datetime.datetime.now())\n",
    "\n",
    "    #result = pd.read_csv(output_path + \"/final4.csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "\n",
    "    u = result.select_dtypes(exclude=['datetime'])\n",
    "    result[u.columns] = u.fillna(0)\n",
    "\n",
    "\n",
    "    plvalue = []\n",
    "    pl = 0\n",
    "    transaction_cost = 0\n",
    "\n",
    "    for idi, row in result.iterrows():\n",
    "        pval = 0\n",
    "        if row[\"D_marker\"] == \"F\":\n",
    "                        \n",
    "            \n",
    "            result.loc[idi, 'pl_check'] = pl + initial_equity\n",
    "            result.loc[idi+1:, 'pl_check'] = np.nan\n",
    "\n",
    "            result.loc[idi , 'units_new'] = ((result.loc[idi ,'pl_check'])/result.loc[idi, 'margin'])\n",
    "            result.loc[idi+1: , 'units_new'] = np.nan\n",
    "\n",
    "            result.loc[idi, index + '_qty_final'] = result.loc[idi, index + \"_qty\"] * result.loc[idi, 'units_new']\n",
    "            result.loc[idi+1:, index + '_qty_final'] = np.nan\n",
    "\n",
    "            result.loc[idi, 'plvalue_' + index] = 0\n",
    "            \n",
    "            contract_value = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi, index + \"_OPT_Close_CE\"] + result.loc[idi, index + \"_OPT_Close_PE\"])\n",
    "            \n",
    "            \n",
    "            for i in stock_list:                                                                    \n",
    "                result.loc[idi, i + '_qty_final'] = result.loc[idi, i + \"_qty\"] * result.loc[idi, 'units_new']\n",
    "                result.loc[idi+1:, i + '_qty_final'] = np.nan\n",
    "                result.loc[idi, 'plvalue_' + i] = 0\n",
    "                contract_value += result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_OPT_Close_CE\"] + result.loc[idi, i + \"_OPT_Close_PE\"])\n",
    "\n",
    "            result.loc[idi, 'transaction_cost'] = 0\n",
    "            result = result.ffill()\n",
    "        \n",
    "\n",
    "        else:\n",
    "\n",
    "                if row[\"Trade_Type\"] == \"SILS\":\n",
    "                    plvalce = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi-1, index + \"_OPT_Close_CE\"] - result.loc[idi, index + \"_OPT_Close_CE\"])\n",
    "                    plvalpe = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi-1, index + \"_OPT_Close_PE\"] - result.loc[idi, index + \"_OPT_Close_PE\"])\n",
    "                    pval = plvalce + plvalpe\n",
    "\n",
    "                    result.loc[idi, 'plvalue_' + index] = plvalce + plvalpe\n",
    "\n",
    "                    for i in stock_list:\n",
    "\n",
    "                        if result.loc[idi-1,i+\"D_marker_2\"] == \"F\":\n",
    "\n",
    "                            plvalce = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_new_OPT_Close_CE\"] - result.loc[idi-1, i + \"_Current_OPT_Close_CE\"])\n",
    "                            plvalpe = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_new_OPT_Close_PE\"] - result.loc[idi-1, i + \"_Current_OPT_Close_PE\"])\n",
    "                            pval += plvalce + plvalpe\n",
    "\n",
    "                            result.loc[idi, 'plvalue_' + i] = plvalce + plvalpe \n",
    "\n",
    "                        else:\n",
    "\n",
    "                            plvalce = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_new_OPT_Close_CE\"] - result.loc[idi-1, i + \"_new_OPT_Close_CE\"])\n",
    "                            plvalpe = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_new_OPT_Close_PE\"] - result.loc[idi-1, i + \"_new_OPT_Close_PE\"])\n",
    "                            pval += plvalce + plvalpe\n",
    "\n",
    "                            result.loc[idi, 'plvalue_' + i] = plvalce + plvalpe\n",
    "                            \n",
    "                        if ((result.loc[idi, i+\"D_marker_1\"] == \"L\") & (result.loc[idi, i+\"D_marker_2\"] == \"F\")):\n",
    "                            \n",
    "                            contract_value += result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_new_OPT_Close_CE\"] + result.loc[idi, i + \"_new_OPT_Close_PE\"])\n",
    "                            contract_value += result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_Current_OPT_Close_CE\"] + result.loc[idi, i + \"_Current_OPT_Close_PE\"])\n",
    "                            \n",
    "                if row[\"Trade_Type\"] == \"LISS\":\n",
    "                    plvalce = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi, index + \"_OPT_Close_CE\"] - result.loc[idi-1, index + \"_OPT_Close_CE\"])\n",
    "                    plvalpe = result.loc[idi, index + \"_qty_final\"]*(result.loc[idi, index + \"_OPT_Close_PE\"] - result.loc[idi-1, index + \"_OPT_Close_PE\"])\n",
    "                    pval = plvalce + plvalpe\n",
    "\n",
    "                    result.loc[idi, 'plvalue_' + index] = plvalce + plvalpe\n",
    "\n",
    "                    for i in stock_list:\n",
    "                        if result.loc[idi-1,i+\"D_marker_2\"] == \"F\":\n",
    "                            plvalce = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_Current_OPT_Close_CE\"] - result.loc[idi, i + \"_new_OPT_Close_CE\"])\n",
    "                            plvalpe = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_Current_OPT_Close_PE\"] - result.loc[idi, i + \"_new_OPT_Close_PE\"])\n",
    "                            pval += plvalce + plvalpe\n",
    "\n",
    "                            result.loc[idi, 'plvalue_' + i] = plvalce + plvalpe \n",
    "\n",
    "                        else:\n",
    "\n",
    "                            plvalce = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_new_OPT_Close_CE\"] - result.loc[idi, i + \"_new_OPT_Close_CE\"])\n",
    "                            plvalpe = result.loc[idi, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_new_OPT_Close_PE\"] - result.loc[idi, i + \"_new_OPT_Close_PE\"])\n",
    "                            pval += plvalce + plvalpe\n",
    "\n",
    "                            result.loc[idi, 'plvalue_' + i] = plvalce + plvalpe \n",
    "                        \n",
    "                        if ((result.loc[idi, i+\"D_marker_1\"] == \"L\") & (result.loc[idi, i+\"D_marker_2\"] == \"F\")):\n",
    "                            \n",
    "                            contract_value += result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_new_OPT_Close_CE\"] + result.loc[idi, i + \"_new_OPT_Close_PE\"])\n",
    "                            contract_value += result.loc[idi, i + \"_qty_final\"]*(result.loc[idi, i + \"_Current_OPT_Close_CE\"] + result.loc[idi, i + \"_Current_OPT_Close_PE\"])\n",
    "\n",
    "\n",
    "\n",
    "                pl += pval\n",
    "\n",
    "        \n",
    "        if row['D_marker'] == \"L\":\n",
    "            \n",
    "            contract_value += result.loc[idi-1, index + \"_qty_final\"]*(result.loc[idi-1, index + \"_OPT_Close_CE\"] + result.loc[idi-1, index + \"_OPT_Close_PE\"])\n",
    "            \n",
    "            for i in stock_list:                                                                    \n",
    "                contract_value += result.loc[idi-1, i + \"_qty_final\"]*(result.loc[idi-1, i + \"_OPT_Close_CE\"] + result.loc[idi-1, i + \"_OPT_Close_PE\"])\n",
    "            \n",
    "            transaction_cost = contract_value*0.005\n",
    "            pl = pl - transaction_cost\n",
    "            result.loc[idi, 'transaction_cost'] = transaction_cost\n",
    "        \n",
    "        plvalue.append(pval)\n",
    "\n",
    "    result[\"Daily_PL\"] = plvalue\n",
    "\n",
    "    #result.to_csv(output_path+\"\\pl_final4.csv\")\n",
    "\n",
    "\n",
    "    result.loc[0,'Daily_PL'] = initial_equity\n",
    "    result[\"PortfolioValue\"] = (result[\"Daily_PL\"] - result[\"transaction_cost\"]).cumsum()\n",
    "    result.loc[0,'Daily_PL'] = 0\n",
    "\n",
    "    #display(result)\n",
    "    result.to_csv(output_path+\"/final4.1__\" + str(t) + \".csv\")\n",
    "    \n",
    "    #######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "    finallist = pd.DataFrame()\n",
    "    tradelist = result[ (result[\"D_marker\"] == \"F\") | (result[\"D_marker\"] == \"L\") ]\n",
    "\n",
    "    if list(tradelist.iloc[[-1]][\"D_marker\"])[0] == \"F\":\n",
    "        tradelist.drop(tradelist.tail(1).index,inplace=True)\n",
    "    else:\n",
    "        tradelist = tradelist\n",
    "\n",
    "    tr1 = tradelist[tradelist[\"D_marker\"]==\"F\"].reset_index().drop(columns = [\"index\"])\n",
    "    tr2 = tradelist[tradelist[\"D_marker\"]==\"L\"].reset_index().drop(columns = [\"index\"])\n",
    "\n",
    "    #print(tr1.shape[0])\n",
    "    if tr1.shape[0] == tr2.shape[0]:\n",
    "        for idx in tr1.index:\n",
    "            mdict = {}\n",
    "            mdict[\"Entry_Date\"] = tr1[index+ \"_Date_CE\"][idx]\n",
    "            mdict[\"Exit_Date\"] = tr2[index+ \"_Date_CE\"][idx]\n",
    "            mdict[\"Trade_Type\"] = tr1[\"Trade_Type\"][idx]\n",
    "            for i in symbols:\n",
    "                for j in [\"CE\",\"PE\"]:\n",
    "                    mdict[i + \"_Entry_Price_\" + j] = tr1[i + \"_OPT_Close_\" + j][idx] \n",
    "                    mdict[i + \"_Exit_Price_\" + j] = tr2[i + \"_OPT_Close_\" + j][idx]\n",
    "                    mdict[i + \"_Qty_\" + j] = tr1[i + \"_qty_final\"][idx]\n",
    "            temp = pd.DataFrame([mdict])\n",
    "            #display(temp)\n",
    "            finallist = finallist.append(temp)\n",
    "    else:\n",
    "        print(\"Error, Tradelist not proper\")\n",
    "\n",
    "    finallist.reset_index().drop(columns = [\"index\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #tradelist =  pd.concat([finallist, finallist_exp], axis=1)\n",
    "\n",
    "    tradelist = finallist\n",
    "\n",
    "    #display(tradelist)\n",
    "\n",
    "    #tradelist.to_csv(output_path+\"/trdlist.csv\")\n",
    "\n",
    "    pl_tradelist = tradelist.copy()\n",
    "    plvalue = []\n",
    "    pval = 0\n",
    "    for idi, row in pl_tradelist.iterrows():\n",
    "\n",
    "        if row[\"Trade_Type\"] == \"SILS\":\n",
    "            pvalce = row[index + \"_Qty_CE\"]*(row[index + \"_Entry_Price_CE\"] - row[index + \"_Exit_Price_CE\"])\n",
    "            pvalpe = row[index + \"_Qty_PE\"]*(row[index + \"_Entry_Price_PE\"] - row[index + \"_Exit_Price_PE\"])\n",
    "            pval = pvalce + pvalpe\n",
    "\n",
    "            for i in stock_list:\n",
    "                pvalce = row[i + \"_Qty_CE\"]*(row[i + \"_Exit_Price_CE\"] - row[i + \"_Entry_Price_CE\"])\n",
    "                pvalpe = row[i + \"_Qty_PE\"]*(row[i + \"_Exit_Price_PE\"] - row[i + \"_Entry_Price_PE\"])\n",
    "                pval += pvalce + pvalpe    \n",
    "\n",
    "        if row[\"Trade_Type\"] == \"LISS\":\n",
    "            pvalce = row[index + \"_Qty_CE\"]*(row[index + \"_Exit_Price_CE\"] - row[index + \"_Entry_Price_CE\"])\n",
    "            pvalpe = row[index + \"_Qty_PE\"]*(row[index + \"_Exit_Price_PE\"] - row[index + \"_Entry_Price_PE\"])\n",
    "            pval = pvalce + pvalpe\n",
    "\n",
    "            for i in stock_list:\n",
    "                pvalce = row[i + \"_Qty_CE\"]*(row[i + \"_Entry_Price_CE\"] - row[i + \"_Exit_Price_CE\"])\n",
    "                pvalpe = row[i + \"_Qty_PE\"]*(row[i + \"_Entry_Price_PE\"] - row[i + \"_Exit_Price_PE\"])\n",
    "                pval += pvalce + pvalpe    \n",
    "\n",
    "        plvalue.append(pval)\n",
    "\n",
    "    pl_tradelist['pl'] = plvalue\n",
    "    pl_tradelist.to_csv(output_path+\"\\pl_trdlist__\" + str(t) + \".csv\")\n",
    "\n",
    "\n",
    "\n",
    "    #######################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### Calculate CAGR \n",
    "    result = pd.read_csv(output_path+\"/final4.1__\" + str(t) + \".csv\", parse_dates = ['BANKNIFTY_Date_CE'], dayfirst = True)\n",
    "    df = result.copy()\n",
    "    from pyxirr import xirr\n",
    "\n",
    "    firstValue = int(df.loc[0, 'PortfolioValue'])\n",
    "    lastValue = int(df.loc[df.index[-1], 'PortfolioValue'])\n",
    "\n",
    "    firstDate = (df.loc[0, 'BANKNIFTY_Date_CE'])\n",
    "    lastDate = (df.loc[df.index[-1], 'BANKNIFTY_Date_CE'])\n",
    "\n",
    "    dates = [firstDate, lastDate]\n",
    "    amounts = [-firstValue, lastValue]\n",
    "    \n",
    "    \n",
    "    xirr = xirr(dates, amounts)\n",
    "\n",
    "    print(\"xirr : \", xirr*100)\n",
    "    ### Calculate Daily Drawdown\n",
    "\n",
    "    Roll_Max = round(df['PortfolioValue'].expanding().max(), 2)\n",
    "    Daily_Drawdown = (round(df['PortfolioValue'], 2)/Roll_Max) - 1.0\n",
    "\n",
    "\n",
    "\n",
    "    df['Daily_Drawdown'] = Daily_Drawdown * 100\n",
    "\n",
    "    print(\"max dd : \", min(df['Daily_Drawdown']))\n",
    "\n",
    "    df.to_csv(output_path + '/DailyDrawdown.csv', index=False)\n",
    "\n",
    "    ### Monthly PNL Percentage \n",
    "\n",
    "    df['BANKNIFTY_curr_exp_date_CE'] = pd.to_datetime(df['BANKNIFTY_curr_exp_date_CE'], dayfirst=True)\n",
    "\n",
    "    df['Year'] = pd.DatetimeIndex(df['BANKNIFTY_curr_exp_date_CE']).year\n",
    "    df['Month'] = pd.DatetimeIndex(df['BANKNIFTY_curr_exp_date_CE']).month\n",
    "    df.to_csv(output_path + '/df.csv', index=False)\n",
    "\n",
    "    i = 0\n",
    "    dfg = df.groupby(['Year', 'Month'])\n",
    "    for name, group in dfg:\n",
    "        #print(name)\n",
    "        if i == 0: \n",
    "            firstValue = group['PortfolioValue'].iloc[0]\n",
    "            i = 1\n",
    "        else:\n",
    "            firstValue = lastValue\n",
    "\n",
    "        lastValue = group['PortfolioValue'].loc[group.index[-1]]\n",
    "        change = lastValue/firstValue - 1\n",
    "        df.loc[group.index, 'Change_%_Monthly'] = round(change * 100, 2)\n",
    "\n",
    "    i = 0\n",
    "    dfg = df.groupby(['Year'])\n",
    "    for name, group in dfg:\n",
    "        #print(name)\n",
    "        if i == 0: \n",
    "            firstValue = group['PortfolioValue'].iloc[0]\n",
    "            i = 1\n",
    "        else:\n",
    "            firstValue = lastValue\n",
    "\n",
    "        lastValue = group['PortfolioValue'].loc[group.index[-1]]\n",
    "        change = lastValue/firstValue - 1\n",
    "        df.loc[group.index, 'Change_%_Yearly'] = change * 100\n",
    "\n",
    "    df\n",
    "\n",
    "    df1 = df[['Year', 'Month', 'Change_%_Monthly', 'Change_%_Yearly']]\n",
    "    df1 = df1.drop_duplicates()\n",
    "    df1\n",
    "\n",
    "    pivotTable = df1.pivot_table(values ='Change_%_Monthly', index =['Year', 'Change_%_Yearly'],\n",
    "                             columns =['Month'])\n",
    "    pivotTable.columns = ['JAN', 'FEB', 'MAR', 'APR', 'MAY', 'JUN',\n",
    "                          'JUL', 'AUG', 'SEP', 'OCT', 'NOV', 'DEC']\n",
    "\n",
    "    pivotTable.index\n",
    "\n",
    "    def _color_red_or_green(val):\n",
    "        color = '#EE0000' if val < 0 else '#00EE00'\n",
    "        return 'background-color: %s' % color\n",
    "    pivotTable.style.applymap(_color_red_or_green)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6316b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0a55bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
