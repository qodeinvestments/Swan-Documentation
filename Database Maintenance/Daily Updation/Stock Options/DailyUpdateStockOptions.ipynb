{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f27ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "from os import walk\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import array_contains\n",
    "# from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_format\n",
    "from datetime import date, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "import psycopg2\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1398b0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Sort_Tuple(tup): \n",
    "    tup.sort(key = lambda x: x[1]) \n",
    "    return tup\n",
    "#path_list = Sort_Tuple(path_list)\n",
    "\n",
    "import re\n",
    "def my_split(s):\n",
    "    return list(filter(None, re.split(r'(\\d+)', s)))\n",
    "#print(path_list)\n",
    "#print(len(path_list))\n",
    "def get_symbol(tic):\n",
    "    li = list(filter(None, re.split(r'(\\d+)', tic)))\n",
    "    return li[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06010988",
   "metadata": {},
   "source": [
    "# Read data from database and convert it into pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4390ec5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-26\n"
     ]
    }
   ],
   "source": [
    "############################################## INPUTS #####################################################\n",
    "startDate = date(2023,4,26)\n",
    "endDate = date(2023,4,26)\n",
    "#startDate = endDate = date.today()\n",
    "\n",
    "total_data_combined = 0\n",
    "ignore_symbols = ['NIFTY', 'BANKNIFTY', 'FINNIFTY']\n",
    "numberOfRowsRaw = 0\n",
    "start_time = datetime.strptime('09:15:00', '%H:%M:%S').time()\n",
    "end_time = datetime.strptime('15:30:00', '%H:%M:%S').time()\n",
    "###########################################################################################################\n",
    "print(startDate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d653d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 09:35:16.958487\n",
      "<pyspark.sql.session.SparkSession object at 0x000001515B59D420>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f73a6880d54bdf97b8ee6b8e5a23f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r26042023\n",
      "2023-04-26\n",
      "Total Data Size 625977\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1ba9458eb44c25aea42e4bafdd48a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4662adf1dc4839807ff0ba6b979629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data appending finished\n",
      "09:35:52\n",
      "Total Rows read from files till now 624978\n",
      "Total Rows in csv files624978\n",
      "2023-04-27 09:35:54.833495\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "\n",
    "# Start spark session\n",
    "spark = SparkSession.builder.config(\"spark.jars\", \"C:\\\\Users\\\\ADMIN\\\\Downloads\\\\postgresql-42.5.0.jar\") \\\n",
    "        .master(\"local\").appName(\"PySpark_Postgres_test\").getOrCreate()\n",
    "print(spark)\n",
    "\n",
    "# Remove previous day files\n",
    "new_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Log_Data\\\\\"\n",
    "log_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Temp_Data\\\\\"\n",
    "\n",
    "for file in next(os.walk(new_data_path))[2]:\n",
    "    os.remove(new_data_path + file)    \n",
    "\n",
    "for file in next(os.walk(log_data_path))[2]:\n",
    "    os.remove(log_data_path + file)\n",
    "    \n",
    "misc_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Temp_Data\\misc_data_afterstockwise\\\\\"\n",
    "log_misc_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Log_Data\\misc_data_afterstockwise\\\\\"\n",
    "\n",
    "for file in next(os.walk(misc_data_path))[2]:\n",
    "    os.remove(log_data_path + file)\n",
    "    \n",
    "for file in next(os.walk(log_misc_data_path))[2]:\n",
    "    os.remove(log_data_path + file)\n",
    "\n",
    "# Loop through all the dates to create symbolwise files\n",
    "for n in tqdm(range(int((endDate - startDate).days)+1)):\n",
    "    currentDate = startDate + timedelta(n)\n",
    "        \n",
    "    day = currentDate.strftime('%d')\n",
    "    nummonth=currentDate.strftime(\"%m\")\n",
    "    year=currentDate.strftime('%Y')\n",
    "    tablename=\"r\"+str(day)+str(nummonth)+str(year)\n",
    "    #print(tablename)\n",
    "\n",
    "    st=time.time()\n",
    "\n",
    "    # Check if file is available in the database\n",
    "    conn = psycopg2.connect(database=\"RawDataBase\",\n",
    "                        user='postgres', password='swancap123',\n",
    "                        host='swandatabase.cfehmk2wtejq.ap-south-1.rds.amazonaws.com', port='5432')\n",
    "                        \n",
    "    cursor = conn.cursor()\n",
    "    stmt = '''Select 1 from rawinfo where name=\\'''' + tablename + '''\\';'''\n",
    "    cursor.execute(stmt)\n",
    "    result = cursor.fetchone()\n",
    "    \n",
    "    if result:\n",
    "        print(tablename)\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    \n",
    "    df = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:postgresql://swandatabase.cfehmk2wtejq.ap-south-1.rds.amazonaws.com/RawDataBase\").option(\"user\",\"postgres\").option(\"password\",\"swancap123\")\\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\").option(\"dbtable\", tablename)\\\n",
    "        .option(\"user\", \"postgres\").option(\"password\", \"swancap123\").load()\n",
    "\n",
    "    # GETTING ONLY TIME IN TIME COLUMN\n",
    "    q = df.withColumn('Time',date_format('Time', 'HH:mm:ss'))\n",
    "\n",
    "    # FILTERING OUT INDEX DATA\n",
    "    bndata = q.filter((~q.ticker.contains('NIFTY')))\n",
    "    bndata = bndata.filter(bndata.ticker.endswith('E.NFO') | bndata.ticker.endswith('E'))\n",
    "\n",
    "    # CONVERTING PYSPARK DATAFRAME TO PANDAS DATAFRAME\n",
    "    bndata=bndata.toPandas()\n",
    "\n",
    "    data = bndata.copy()\n",
    "    data.drop(data.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "    \n",
    "    data = data.rename(columns={'Ticker ' : 'Ticker',\n",
    "                                'ticker' : 'Ticker',\n",
    "                                'date' : 'Date',\n",
    "                                'time' : 'Time',\n",
    "                                'open' : 'Open',\n",
    "                                'high' : 'High',\n",
    "                                'low' : 'Low',\n",
    "                                'close' : 'Close',\n",
    "                                'volume' : 'Volume',\n",
    "                                'open interest' : 'Open Interest'})\n",
    "    \n",
    "    data[\"Ticker\"] = data[\"Ticker\"].str.replace(\"HINDALC0\",\"HINDALCO\")\n",
    "    data[\"Ticker\"] = data[\"Ticker\"].str.replace(\"IBN18\",\"IBN_ET\")\n",
    "    data[\"Ticker\"] = data[\"Ticker\"].str.replace(\"TV-18\",\"TV_ET\")\n",
    "    data[\"Ticker\"] = data[\"Ticker\"].str.replace(\"NETWORK18\",\"NETWORK_ET\")\n",
    "    data[\"Ticker\"] = data[\"Ticker\"].str.replace(\"TV18BRDCST\",\"TV_ETBRDCST\")\n",
    "    data[\"Ticker\"] = data[\"Ticker\"].str.replace(\"3IINFOTECH\",\"TH_IINFOTECH\")\n",
    "    data[\"Ticker\"] = data[\"Ticker\"].str.replace(\"BAJAJ-AUTO\",\"BAJAJ_AUTO\")\n",
    "    data[\"New_date\"] = currentDate\n",
    "    print(currentDate)\n",
    "    print(\"Total Data Size \"+ str(data.shape[0]))\n",
    "    \n",
    "    data[\"symbol\"] = data[\"Ticker\"].str.split(\"(\\d+)\").str[0]\n",
    "    \n",
    "    fut_data = data[data[\"Ticker\"].str.endswith((\"I\",\"II\",\"III\", 'I.NFO', 'II.NFO', 'III.NFO'))] \n",
    "    data = data[~data[\"Ticker\"].str.endswith((\"I\",\"II\",\"III\"))]\n",
    "    misc_data = data[~data[\"Ticker\"].str.endswith(('.NFO', 'CE', 'PE'))]\n",
    "    data = data[data[\"Ticker\"].str.endswith((\"PE\",\"CE\",\"PE.NFO\",\"CE.NFO\"))]\n",
    "    \n",
    "    \n",
    "    data = data[~data['symbol'].isin(ignore_symbols)]\n",
    "    data['Time'] = pd.to_datetime(data['Time']).dt.time\n",
    "    data = data[(data['Time']>=start_time) & (data['Time']<=end_time)]\n",
    "    \n",
    "    numberOfRowsRaw += data.shape[0]\n",
    "    total_data_combined = total_data_combined + data.shape[0]\n",
    "       \n",
    "    gb = data.groupby([\"symbol\"])\n",
    "    df_list = [gb.get_group(x) for x in gb.groups]\n",
    "       \n",
    "    for tup in tqdm(gb.groups):\n",
    "        temp = gb.get_group(tup)\n",
    "        \n",
    "        temp.to_csv(new_data_path + str(tup)  + '.csv', mode='a', header=not os.path.exists(new_data_path + str(tup)  + '.csv'), index=False)\n",
    "        temp.to_csv(log_data_path + str(tup)  + '.csv', mode='a', header=not os.path.exists(log_data_path + str(tup)  + '.csv'), index=False)   \n",
    "\n",
    "    print(\"Processing Finished\")  \n",
    "        \n",
    "    gb = misc_data.groupby([\"symbol\"])\n",
    "    df_list = [gb.get_group(x) for x in gb.groups]\n",
    "       \n",
    "    for tup in tqdm(gb.groups):\n",
    "        temp = gb.get_group(tup)       \n",
    "        \n",
    "        temp.to_csv(misc_data_path + str(tup) + '.csv', mode='a', header=not os.path.exists(misc_data_path + str(tup) + '.csv'), index=False)\n",
    "        temp.to_csv(log_misc_data_path + str(tup)  + '.csv', mode='a', header=not os.path.exists(log_misc_data_path + str(tup)  + '.csv'), index=False)\n",
    "     \n",
    "    print(\"Data appending finished\")\n",
    "    t = time.localtime()\n",
    "    current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "    print(current_time)\n",
    "    \n",
    "    print(\"Total Rows read from files till now \" + str(total_data_combined))\n",
    "    \n",
    "    mypath = new_data_path\n",
    "    filename = next(walk(mypath), (None, None, []))[2]  # [] if no file\n",
    "    final = []\n",
    "    for i in filename:\n",
    "        temp = mypath +\"/\"+ i\n",
    "        final.append(temp)\n",
    "    sum1 = 0\n",
    "    for i in final:\n",
    "        df = pd.read_csv(i)\n",
    "        sum1 += df.shape[0]\n",
    "    print(\"Total Rows in csv files\" + str(sum1))\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c36af",
   "metadata": {},
   "source": [
    "# Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cef3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = datetime.strptime('30-12-2019', '%d-%m-%Y')\n",
    "new_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Temp_Data\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2526862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 09:45:12.140422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36aeea35db1e47f3b5d8282e11f8cb73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AARTIIND\n",
      "ABB\n",
      "ABBOTINDIA\n",
      "ABCAPITAL\n",
      "ABFRL\n",
      "ACC\n",
      "ADANIENT\n",
      "ADANIPORTS\n",
      "ALKEM\n",
      "AMBUJACEM\n",
      "APOLLOHOSP\n",
      "APOLLOTYRE\n",
      "ASHOKLEY\n",
      "ASIANPAINT\n",
      "ASTRAL\n",
      "ATUL\n",
      "AUBANK\n",
      "AUROPHARMA\n",
      "AXISBANK\n",
      "BAJAJFINSV\n",
      "BAJAJ_AUTO\n",
      "BAJFINANCE\n",
      "BALKRISIND\n",
      "BALRAMCHIN\n",
      "BANDHANBNK\n",
      "BANKBARODA\n",
      "BATAINDIA\n",
      "BEL\n",
      "BERGEPAINT\n",
      "BHARATFORG\n",
      "BHARTIARTL\n",
      "BHEL\n",
      "BIOCON\n",
      "BOSCHLTD\n",
      "BPCL\n",
      "BRITANNIA\n",
      "BSOFT\n",
      "CANBK\n",
      "CANFINHOME\n",
      "CHAMBLFERT\n",
      "CHOLAFIN\n",
      "CIPLA\n",
      "COALINDIA\n",
      "COFORGE\n",
      "COLPAL\n",
      "CONCOR\n",
      "COROMANDEL\n",
      "CROMPTON\n",
      "CUB\n",
      "CUMMINSIND\n",
      "DABUR\n",
      "DALBHARAT\n",
      "DEEPAKNTR\n",
      "DELTACORP\n",
      "DIVISLAB\n",
      "DIXON\n",
      "DLF\n",
      "DRREDDY\n",
      "EICHERMOT\n",
      "ESCORTS\n",
      "EXIDEIND\n",
      "FEDERALBNK\n",
      "GAIL\n",
      "GLENMARK\n",
      "GMRINFRA\n",
      "GNFC\n",
      "GODREJCP\n",
      "GODREJPROP\n",
      "GRANULES\n",
      "GRASIM\n",
      "GUJGASLTD\n",
      "HAL\n",
      "HAVELLS\n",
      "HCLTECH\n",
      "HDFC\n",
      "HDFCAMC\n",
      "HDFCBANK\n",
      "HDFCLIFE\n",
      "HEROMOTOCO\n",
      "HINDALCO\n",
      "HINDCOPPER\n",
      "HINDPETRO\n",
      "HINDUNILVR\n",
      "HONAUT\n",
      "IBULHSGFIN\n",
      "ICICIBANK\n",
      "ICICIGI\n",
      "ICICIPRULI\n",
      "IDEA\n",
      "IDFC\n",
      "IDFCFIRSTB\n",
      "IEX\n",
      "IGL\n",
      "INDHOTEL\n",
      "INDIACEM\n",
      "INDIAMART\n",
      "INDIGO\n",
      "INDUSINDBK\n",
      "INDUSTOWER\n",
      "INFY\n",
      "INTELLECT\n",
      "IOC\n",
      "IPCALAB\n",
      "IRCTC\n",
      "ITC\n",
      "JINDALSTEL\n",
      "JKCEMENT\n",
      "JSWSTEEL\n",
      "JUBLFOOD\n",
      "KOTAKBANK\n",
      "L&TFH\n",
      "LALPATHLAB\n",
      "LAURUSLABS\n",
      "LICHSGFIN\n",
      "LT\n",
      "LTIM\n",
      "LTTS\n",
      "LUPIN\n",
      "M&M\n",
      "M&MFIN\n",
      "MANAPPURAM\n",
      "MARICO\n",
      "MARUTI\n",
      "MCDOWELL-N\n",
      "MCX\n",
      "METROPOLIS\n",
      "MFSL\n",
      "MGL\n",
      "MOTHERSON\n",
      "MPHASIS\n",
      "MRF\n",
      "MUTHOOTFIN\n",
      "NATIONALUM\n",
      "NAUKRI\n",
      "NAVINFLUOR\n",
      "NESTLEIND\n",
      "NMDC\n",
      "NTPC\n",
      "OBEROIRLTY\n",
      "OFSS\n",
      "ONGC\n",
      "PAGEIND\n",
      "PEL\n",
      "PERSISTENT\n",
      "PETRONET\n",
      "PFC\n",
      "PIDILITIND\n",
      "PIIND\n",
      "PNB\n",
      "POLYCAB\n",
      "POWERGRID\n",
      "PVR\n",
      "RAIN\n",
      "RAMCOCEM\n",
      "RBLBANK\n",
      "RECLTD\n",
      "RELIANCE\n",
      "SAIL\n",
      "SBICARD\n",
      "SBILIFE\n",
      "SBIN\n",
      "SHREECEM\n",
      "SHRIRAMFIN\n",
      "SIEMENS\n",
      "SRF\n",
      "SUNPHARMA\n",
      "SUNTV\n",
      "SYNGENE\n",
      "TATACHEM\n",
      "TATACOMM\n",
      "TATACONSUM\n",
      "TATAMOTORS\n",
      "TATAPOWER\n",
      "TATASTEEL\n",
      "TCS\n",
      "TECHM\n",
      "TITAN\n",
      "TORNTPHARM\n",
      "TRENT\n",
      "TVSMOTOR\n",
      "UBL\n",
      "ULTRACEMCO\n",
      "UPL\n",
      "VEDL\n",
      "VOLTAS\n",
      "WHIRLPOOL\n",
      "WIPRO\n",
      "ZEEL\n",
      "ZYDUSLIFE\n",
      "2023-04-27 09:45:32.845356\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "mypath = new_data_path\n",
    "filename = next(walk(mypath), (None, None, []))[2]  # [] if no file\n",
    "file_path = {}\n",
    "for i in filename:\n",
    "    temp = mypath +\"/\"+ i\n",
    "    file_path[i.replace(\".csv\",\"\")] = temp\n",
    "\n",
    "labeled_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Temp_Data\\labeled_data\\\\\"\n",
    "log_labeled_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Log_Data\\labeled_data\\\\\"\n",
    "\n",
    "for i in next(os.walk(labeled_data_path))[2]:\n",
    "    os.remove(labeled_data_path + i)\n",
    "\n",
    "for i in next(os.walk(log_labeled_data_path))[2]:\n",
    "    os.remove(log_labeled_data_path + i)\n",
    "\n",
    "for sym in tqdm(sorted(file_path)):\n",
    "    print(sym)\n",
    "    data = pd.read_csv(file_path[sym], chunksize = 100000)\n",
    "\n",
    "    for df in data:\n",
    "        df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "        df = df.drop_duplicates()\n",
    "        df = df.drop(columns = [\"symbol\"])\n",
    "        \n",
    "        df['Date'] = pd.to_datetime(df['Date'], dayfirst=True)\n",
    "        df['Ticker'] = df['Ticker'].str.replace('.NFO', '')\n",
    "        df[\"Symbol\"] = sym\n",
    "        df[\"Option_type\"] = df[\"Ticker\"].str[-2:]\n",
    "        df[\"Temp\"] = df[\"Ticker\"].str.replace(sym,\"\")        \n",
    "        df[\"Temp\"] = df[\"Temp\"].str[:-2]\n",
    "        \n",
    "        df['Exp_year'] = np.where(df['Date']>=date1, df[\"Temp\"].str[5:7], df['Temp'].str[0:2])\n",
    "        #df[\"Exp_year\"] = df[\"Temp\"].str[5:7] # change Exp_year\n",
    "        df[\"Exp_month\"] = df[\"Temp\"].str[2:5]\n",
    "        df['Strike'] = np.where(df['Date']>=date1, df[\"Temp\"].str[7:], df['Temp'].str[5:])\n",
    "        \n",
    "        df = df.rename(columns={'Time' : 'Timestamp'})\n",
    "        df['New_date_Time'] = pd.to_datetime(df['New_date'] + ' ' + df['Timestamp'])\n",
    "        \n",
    "        #df[\"Strike\"] = df[\"Temp\"].str[7:] # change Strike\n",
    "        df.to_csv(labeled_data_path + sym + '.csv', mode='a', header=not os.path.exists(labeled_data_path + sym + '.csv'), index=False)    \n",
    "        df.to_csv(log_labeled_data_path + sym + '.csv', mode='a', header=not os.path.exists(log_labeled_data_path + sym + '.csv'), index=False)\n",
    "        \n",
    "        del df\n",
    "        \n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491586b",
   "metadata": {},
   "source": [
    "# Continuous Contracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e20a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(stri):\n",
    "    obj = datetime.strptime(stri, \"%b\")\n",
    "    month_number = obj.month\n",
    "    return month_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63cb06c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "mypath = labeled_data_path\n",
    "filename = next(walk(mypath), (None, None, []))[2]  # [] if no file\n",
    "file_path = {}\n",
    "\n",
    "for i in filename:\n",
    "    temp = mypath +\"/\"+ i\n",
    "    file_path[i.replace(\".csv\",\"\")] = temp\n",
    "\n",
    "exp_file_path = r\"E:\\sourav\\Database\\Codes\\CsvFiles\\MonthlyExpiry.csv\"\n",
    "exp_df = pd.read_csv(exp_file_path,parse_dates = [\"curr_exp_date\",\"curr_date\"],dayfirst =True,usecols = [\"curr_exp_date\",\"curr_date\"]).dropna()\n",
    "exp_df.rename({'curr_date': 'New_date'}, axis=1, inplace=True)\n",
    "\n",
    "print(exp_df.isnull().values.any())\n",
    "\n",
    "folpath = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Temp_Data\\cont_data\\\\\"\n",
    "folpath_log = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Log_Data\\cont_data\\\\\"\n",
    "diff_list_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "408d7707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-27 09:45:55.789767\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00072c0914f14c8594c80f528c9769f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AARTIIND\n",
      "1021\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1021\n",
      "ABB\n",
      "3176\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3176\n",
      "ABBOTINDIA\n",
      "241\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "241\n",
      "ABCAPITAL\n",
      "1321\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1321\n",
      "ABFRL\n",
      "924\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "924\n",
      "ACC\n",
      "2308\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2308\n",
      "ADANIENT\n",
      "15827\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "15827\n",
      "ADANIPORTS\n",
      "6919\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "6919\n",
      "ALKEM\n",
      "355\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "355\n",
      "AMBUJACEM\n",
      "3431\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3431\n",
      "APOLLOHOSP\n",
      "5988\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5988\n",
      "APOLLOTYRE\n",
      "1677\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1677\n",
      "ASHOKLEY\n",
      "2525\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2525\n",
      "ASIANPAINT\n",
      "6448\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "6448\n",
      "ASTRAL\n",
      "575\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "575\n",
      "ATUL\n",
      "326\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "326\n",
      "AUBANK\n",
      "7666\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "7666\n",
      "AUROPHARMA\n",
      "5848\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5848\n",
      "AXISBANK\n",
      "9433\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "9433\n",
      "BAJAJFINSV\n",
      "5233\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5233\n",
      "BAJAJ_AUTO\n",
      "11997\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "11997\n",
      "BAJFINANCE\n",
      "15728\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "15728\n",
      "BALKRISIND\n",
      "566\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "566\n",
      "BALRAMCHIN\n",
      "2116\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2116\n",
      "BANDHANBNK\n",
      "3273\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3273\n",
      "BANKBARODA\n",
      "4535\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4535\n",
      "BATAINDIA\n",
      "1515\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1515\n",
      "BEL\n",
      "1763\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1763\n",
      "BERGEPAINT\n",
      "1558\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1558\n",
      "BHARATFORG\n",
      "1204\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1204\n",
      "BHARTIARTL\n",
      "5043\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5043\n",
      "BHEL\n",
      "1981\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1981\n",
      "BIOCON\n",
      "2399\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2399\n",
      "BOSCHLTD\n",
      "450\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "450\n",
      "BPCL\n",
      "2308\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2308\n",
      "BRITANNIA\n",
      "2571\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2571\n",
      "BSOFT\n",
      "810\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "810\n",
      "CANBK\n",
      "3743\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3743\n",
      "CANFINHOME\n",
      "1939\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1939\n",
      "CHAMBLFERT\n",
      "2924\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2924\n",
      "CHOLAFIN\n",
      "1410\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1410\n",
      "CIPLA\n",
      "2784\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2784\n",
      "COALINDIA\n",
      "2930\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2930\n",
      "COFORGE\n",
      "5435\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5435\n",
      "COLPAL\n",
      "459\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "459\n",
      "CONCOR\n",
      "1719\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1719\n",
      "COROMANDEL\n",
      "408\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "408\n",
      "CROMPTON\n",
      "4469\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4469\n",
      "CUB\n",
      "3111\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3111\n",
      "CUMMINSIND\n",
      "935\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "935\n",
      "DABUR\n",
      "2678\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2678\n",
      "DALBHARAT\n",
      "2098\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2098\n",
      "DEEPAKNTR\n",
      "1960\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1960\n",
      "DELTACORP\n",
      "945\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "945\n",
      "DIVISLAB\n",
      "6483\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "6483\n",
      "DIXON\n",
      "2380\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2380\n",
      "DLF\n",
      "4663\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4663\n",
      "DRREDDY\n",
      "3725\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3725\n",
      "EICHERMOT\n",
      "4839\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4839\n",
      "ESCORTS\n",
      "1789\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1789\n",
      "EXIDEIND\n",
      "1045\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1045\n",
      "FEDERALBNK\n",
      "2776\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2776\n",
      "GAIL\n",
      "1155\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1155\n",
      "GLENMARK\n",
      "1395\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1395\n",
      "GMRINFRA\n",
      "954\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "954\n",
      "GNFC\n",
      "9298\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "9298\n",
      "GODREJCP\n",
      "1212\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1212\n",
      "GODREJPROP\n",
      "2255\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2255\n",
      "GRANULES\n",
      "484\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "484\n",
      "GRASIM\n",
      "1185\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1185\n",
      "GUJGASLTD\n",
      "2194\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2194\n",
      "HAL\n",
      "5439\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5439\n",
      "HAVELLS\n",
      "2185\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2185\n",
      "HCLTECH\n",
      "5476\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5476\n",
      "HDFC\n",
      "3747\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3747\n",
      "HDFCAMC\n",
      "2715\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2715\n",
      "HDFCBANK\n",
      "11958\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "11958\n",
      "HDFCLIFE\n",
      "7030\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "7030\n",
      "HEROMOTOCO\n",
      "4591\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4591\n",
      "HINDALCO\n",
      "4099\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4099\n",
      "HINDCOPPER\n",
      "474\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "474\n",
      "HINDPETRO\n",
      "821\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "821\n",
      "HINDUNILVR\n",
      "4741\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4741\n",
      "HONAUT\n",
      "26\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "26\n",
      "IBULHSGFIN\n",
      "1532\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1532\n",
      "ICICIBANK\n",
      "9436\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "9436\n",
      "ICICIGI\n",
      "1770\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1770\n",
      "ICICIPRULI\n",
      "2748\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2748\n",
      "IDEA\n",
      "632\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "632\n",
      "IDFC\n",
      "761\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "761\n",
      "IDFCFIRSTB\n",
      "2366\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2366\n",
      "IEX\n",
      "2111\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2111\n",
      "IGL\n",
      "3210\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3210\n",
      "INDHOTEL\n",
      "3103\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3103\n",
      "INDIACEM\n",
      "1344\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1344\n",
      "INDIAMART\n",
      "1616\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1616\n",
      "INDIGO\n",
      "2251\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2251\n",
      "INDUSINDBK\n",
      "9729\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "9729\n",
      "INDUSTOWER\n",
      "6239\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "6239\n",
      "INFY\n",
      "12775\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "12775\n",
      "INTELLECT\n",
      "106\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "106\n",
      "IOC\n",
      "1169\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1169\n",
      "IPCALAB\n",
      "6805\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "6805\n",
      "IRCTC\n",
      "3693\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3693\n",
      "ITC\n",
      "8626\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "8626\n",
      "JINDALSTEL\n",
      "2988\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2988\n",
      "JKCEMENT\n",
      "344\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "344\n",
      "JSWSTEEL\n",
      "2742\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2742\n",
      "JUBLFOOD\n",
      "2481\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2481\n",
      "KOTAKBANK\n",
      "7550\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "7550\n",
      "L&TFH\n",
      "1181\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1181\n",
      "LALPATHLAB\n",
      "777\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "777\n",
      "LAURUSLABS\n",
      "3965\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3965\n",
      "LICHSGFIN\n",
      "1597\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1597\n",
      "LT\n",
      "8436\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "8436\n",
      "LTIM\n",
      "3678\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3678\n",
      "LTTS\n",
      "3681\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3681\n",
      "LUPIN\n",
      "1861\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1861\n",
      "M&M\n",
      "3429\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3429\n",
      "M&MFIN\n",
      "1492\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1492\n",
      "MANAPPURAM\n",
      "1171\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1171\n",
      "MARICO\n",
      "2645\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2645\n",
      "MARUTI\n",
      "11568\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "11568\n",
      "MCDOWELL-N\n",
      "1983\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1983\n",
      "MCX\n",
      "3198\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3198\n",
      "METROPOLIS\n",
      "917\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "917\n",
      "MFSL\n",
      "1172\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1172\n",
      "MGL\n",
      "1172\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1172\n",
      "MOTHERSON\n",
      "1419\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1419\n",
      "MPHASIS\n",
      "1689\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1689\n",
      "MRF\n",
      "475\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "475\n",
      "MUTHOOTFIN\n",
      "1450\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1450\n",
      "NATIONALUM\n",
      "787\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "787\n",
      "NAUKRI\n",
      "1362\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1362\n",
      "NAVINFLUOR\n",
      "973\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "973\n",
      "NESTLEIND\n",
      "5113\n",
      "Sanity Check Success\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check2 Success\n",
      "5113\n",
      "NMDC\n",
      "1195\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1195\n",
      "NTPC\n",
      "1576\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1576\n",
      "OBEROIRLTY\n",
      "1079\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1079\n",
      "OFSS\n",
      "564\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "564\n",
      "ONGC\n",
      "2372\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2372\n",
      "PAGEIND\n",
      "1030\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1030\n",
      "PEL\n",
      "2472\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2472\n",
      "PERSISTENT\n",
      "5506\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "5506\n",
      "PETRONET\n",
      "665\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "665\n",
      "PFC\n",
      "3318\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3318\n",
      "PIDILITIND\n",
      "1636\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1636\n",
      "PIIND\n",
      "1808\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1808\n",
      "PNB\n",
      "2525\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2525\n",
      "POLYCAB\n",
      "1870\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1870\n",
      "POWERGRID\n",
      "3474\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3474\n",
      "PVR\n",
      "3816\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3816\n",
      "RAIN\n",
      "314\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "314\n",
      "RAMCOCEM\n",
      "471\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "471\n",
      "RBLBANK\n",
      "1705\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1705\n",
      "RECLTD\n",
      "1504\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1504\n",
      "RELIANCE\n",
      "16326\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "16326\n",
      "SAIL\n",
      "1562\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1562\n",
      "SBICARD\n",
      "1890\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1890\n",
      "SBILIFE\n",
      "3169\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3169\n",
      "SBIN\n",
      "11773\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "11773\n",
      "SHREECEM\n",
      "2622\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2622\n",
      "SHRIRAMFIN\n",
      "2037\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2037\n",
      "SIEMENS\n",
      "4802\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4802\n",
      "SRF\n",
      "1306\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1306\n",
      "SUNPHARMA\n",
      "2669\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2669\n",
      "SUNTV\n",
      "331\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "331\n",
      "SYNGENE\n",
      "764\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "764\n",
      "TATACHEM\n",
      "4504\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4504\n",
      "TATACOMM\n",
      "1533\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1533\n",
      "TATACONSUM\n",
      "9383\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "9383\n",
      "TATAMOTORS\n",
      "9209\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "9209\n",
      "TATAPOWER\n",
      "4664\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4664\n",
      "TATASTEEL\n",
      "4808\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4808\n",
      "TCS\n",
      "11215\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "11215\n",
      "TECHM\n",
      "6135\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "6135\n",
      "TITAN\n",
      "4279\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4279\n",
      "TORNTPHARM\n",
      "457\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "457\n",
      "TRENT\n",
      "1052\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1052\n",
      "TVSMOTOR\n",
      "3612\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "3612\n",
      "UBL\n",
      "824\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "824\n",
      "ULTRACEMCO\n",
      "4472\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4472\n",
      "UPL\n",
      "2663\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "2663\n",
      "VEDL\n",
      "4434\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "4434\n",
      "VOLTAS\n",
      "7712\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "7712\n",
      "WHIRLPOOL\n",
      "419\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "419\n",
      "WIPRO\n",
      "6015\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "6015\n",
      "ZEEL\n",
      "1194\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "1194\n",
      "ZYDUSLIFE\n",
      "785\n",
      "Sanity Check Success\n",
      "Sanity check2 Success\n",
      "785\n",
      "2023-04-27 09:46:31.092322\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())\n",
    "\n",
    "for file in next(os.walk(folpath))[2]:\n",
    "    os.remove(folpath + file)\n",
    "\n",
    "for file in next(os.walk(folpath_log))[2]:\n",
    "    os.remove(folpath_log + file)\n",
    "\n",
    "for sym in tqdm(sorted(file_path)):\n",
    "    \n",
    "    print(sym)\n",
    "    df = pd.read_csv(file_path[sym],parse_dates = [\"New_date\"],dayfirst =True)\n",
    "\n",
    "    if df.empty == True:\n",
    "        continue\n",
    "        \n",
    "    if sym == 'S&P':\n",
    "        continue\n",
    "        \n",
    "    df['exp_month_number'] = df.apply(lambda row : add(row[\"Exp_month\"]), axis = 1)\n",
    "    df[\"New_date\"] = pd.to_datetime(df[\"New_date\"], dayfirst=True)\n",
    "    df[\"current_month_number\"] = df['New_date'].dt.month\n",
    "    df[\"difference\"] = df['exp_month_number'].astype(int) - df[\"current_month_number\"].astype(int)\n",
    "    \n",
    "    df1 = pd.merge(df, exp_df, on ='New_date', how ='left')\n",
    "    \n",
    "\n",
    "    df1.drop(df1.filter(regex=\"Unname\"),axis=1, inplace=True)\n",
    "    print(df1.shape[0])\n",
    "    df1[\"current_exp_month_number\"] = df1['curr_exp_date'].dt.month\n",
    "    df1[\"Diff_months\"] = df1[\"current_exp_month_number\"] - df1[\"current_month_number\"]\n",
    "    \n",
    "\n",
    "    df1[\"Diff_months\"] = df1[\"Diff_months\"].astype(int) \n",
    "\n",
    "    bdf = df1[df1[\"Diff_months\"] == 0]\n",
    "    adf = df1[(df1[\"Diff_months\"] == 1) | (df1[\"Diff_months\"] == -11)]\n",
    "    \n",
    "    if bdf.shape[0] + adf.shape[0] == df1.shape[0]:\n",
    "        print(\"Sanity Check Success\")\n",
    "    else:\n",
    "        print(\"Error1\")\n",
    "        break\n",
    "    \n",
    "    agb = adf.groupby([\"difference\"])\n",
    "    unique_val_list_a = list(adf[\"difference\"].unique())\n",
    "    bgb = bdf.groupby([\"difference\"])\n",
    "    unique_val_list_b = list(bdf[\"difference\"].unique())\n",
    "\n",
    "    for i in unique_val_list_b:\n",
    "        temp_df = bgb.get_group(i)\n",
    "        \n",
    "        if i == 0:\n",
    "            temp_df.to_csv(folpath + sym + '-I.csv', mode='a', header=not os.path.exists(folpath + sym + '-I.csv'), index=False)\n",
    "            temp_df.to_csv(folpath_log + sym + '-I.csv', mode='a', header=not os.path.exists(folpath_log + sym + '-I.csv'), index=False) \n",
    "        \n",
    "        elif i == 1 or i == -11:\n",
    "            temp_df.to_csv(folpath + sym + '-II.csv', mode='a', header=not os.path.exists(folpath + sym + '-II.csv'), index=False)\n",
    "            temp_df.to_csv(folpath_log + sym + '-II.csv', mode='a', header=not os.path.exists(folpath_log + sym + '-II.csv'), index=False)\n",
    "        \n",
    "        elif i == 2 or i == -10:\n",
    "            temp_df.to_csv(folpath + sym + '-III.csv', mode='a', header=not os.path.exists(folpath + sym + '-III.csv'), index=False)\n",
    "            temp_df.to_csv(folpath_log + sym + '-III.csv', mode='a', header=not os.path.exists(folpath_log + sym + '-III.csv'), index=False)\n",
    "\n",
    "        else:\n",
    "            temp_df.to_csv(folpath + sym + 'misc.csv', mode='a', header=not os.path.exists(folpath + sym + 'misc.csv'), index=False)\n",
    "            temp_df.to_csv(folpath_log + sym + 'misc.csv', mode='a', header=not os.path.exists(folpath_log + sym + 'misc.csv'), index=False)\n",
    "         \n",
    "    for i in unique_val_list_a:\n",
    "        temp_df = agb.get_group(i)\n",
    "        \n",
    "        if i == 1 or i == -11:\n",
    "            temp_df.to_csv(folpath + sym + '-I.csv', mode='a', header=not os.path.exists(folpath + sym + '-I.csv'), index=False)\n",
    "            temp_df.to_csv(folpath_log + sym + '-I.csv', mode='a', header=not os.path.exists(folpath_log + sym + '-I.csv'), index=False)\n",
    "    \n",
    "        elif i == 2 or i == -10:\n",
    "            temp_df.to_csv(folpath + sym + '-II.csv', mode='a', header=not os.path.exists(folpath + sym + '-II.csv'), index=False)\n",
    "            temp_df.to_csv(folpath_log + sym + '-II.csv', mode='a', header=not os.path.exists(folpath_log + sym + '-II.csv'), index=False)\n",
    "                           \n",
    "        elif i == 3 or i == -9:\n",
    "            temp_df.to_csv(folpath + sym + '-III.csv', mode='a', header=not os.path.exists(folpath + sym + '-III.csv'), index=False)               \n",
    "            temp_df.to_csv(folpath_log + sym + '-III.csv', mode='a', header=not os.path.exists(folpath_log + sym + '-III.csv'), index=False)               \n",
    "                        \n",
    "        else:\n",
    "            temp_df.to_csv(folpath + sym +'misc.csv', mode='a', header=not os.path.exists(folpath + sym +'misc.csv'), index=False)\n",
    "            temp_df.to_csv(folpath_log + sym + 'misc.csv', mode='a', header=not os.path.exists(folpath_log + sym + 'misc.csv'), index=False)               \n",
    "            \n",
    "        \n",
    "    list_files = os.listdir(folpath)\n",
    "    sum1 = 0\n",
    "    for i in list_files:\n",
    "        if i.startswith(sym):\n",
    "            dff = pd.read_csv(folpath + i)\n",
    "            sum1 += dff.shape[0]\n",
    "    if sum1 == df1.shape[0]:\n",
    "        print(\"Sanity check2 Success\")\n",
    "    else:\n",
    "        print(\"Error2\")\n",
    "        break\n",
    "    print(sum1)\n",
    "    \n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09434e9d",
   "metadata": {},
   "source": [
    "# Create Adj Columns and Append to Main Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81bdb8c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Temp_Data\\cont_data\\\\\"\n",
    "#output_path = r\"E:\\sourav\\Database\\OptionsDatabaseDailyUpdate\\Updation_Temp_Data\\split_adjusted_final\"\n",
    "#output_path = r\"D:\\Sourav\\Data\\UpdatedTill23Dec2022\\\\\"\n",
    "output_path = r\"D:\\Sourav\\Data\\UpdatedTill23Dec2022 - 1\\\\\"\n",
    "symbols = next(os.walk(final_data_path))[2]\n",
    "len(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3425a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    }
   ],
   "source": [
    "#symbols = ['HINDPETRO']\n",
    "for i in range(len(symbols)):\n",
    "    symbols[i] = symbols[i].replace('-III.csv', '').replace('-II.csv', '').replace('-I.csv', '')    \n",
    "symbols = sorted(list(set(symbols)))\n",
    "print(len(symbols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "560d1d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af722a5a056d40c6a0617465a31f66b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/189 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AARTIIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ABB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ABBOTINDIA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ABCAPITAL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ABFRL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ACC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ADANIENT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ADANIPORTS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ALKEM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "AMBUJACEM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "APOLLOHOSP\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "APOLLOTYRE\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ASHOKLEY\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ASIANPAINT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ASTRAL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ATUL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "AUBANK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "AUROPHARMA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "AXISBANK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BAJAJFINSV\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BAJAJ_AUTO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BAJFINANCE\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BALKRISIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BALRAMCHIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BANDHANBNK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BANKBARODA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BATAINDIA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BERGEPAINT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BHARATFORG\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BHARTIARTL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BHEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BIOCON\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BOSCHLTD\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BPCL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BRITANNIA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "BSOFT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CANBK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CANFINHOME\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CHAMBLFERT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CHOLAFIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CIPLA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "COALINDIA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "COFORGE\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "COLPAL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CONCOR\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "COROMANDEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CROMPTON\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CUB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "CUMMINSIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DABUR\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DALBHARAT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DEEPAKNTR\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DELTACORP\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DIVISLAB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DIXON\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DLF\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "DRREDDY\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "EICHERMOT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ESCORTS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "EXIDEIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "FEDERALBNK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GAIL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GLENMARK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GMRINFRA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GNFC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GODREJCP\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GODREJPROP\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GRANULES\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GRASIM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "GUJGASLTD\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HAL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HAVELLS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HCLTECH\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HDFC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HDFCAMC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HDFCBANK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HDFCLIFE\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HEROMOTOCO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HINDALCO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HINDCOPPER\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HINDPETRO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HINDUNILVR\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "HONAUT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IBULHSGFIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ICICIBANK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ICICIGI\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ICICIPRULI\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IDEA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IDFC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IDFCFIRSTB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IEX\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IGL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INDHOTEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INDIACEM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INDIAMART\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INDIGO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INDUSINDBK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INDUSTOWER\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INFY\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "INTELLECT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IOC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IPCALAB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "IRCTC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ITC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "JINDALSTEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "JKCEMENT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "JSWSTEEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "JUBLFOOD\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "KOTAKBANK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "L&TFH\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "LALPATHLAB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "LAURUSLABS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "LICHSGFIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "LT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "LTIM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "LTTS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "LUPIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "M&M\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "M&MFIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MANAPPURAM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MARICO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MARUTI\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MCDOWELL-N\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MCX\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "METROPOLIS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MFSL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MGL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MOTHERSON\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MPHASIS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MRF\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "MUTHOOTFIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "NATIONALUM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "NAUKRI\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "NAVINFLUOR\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "NESTLEIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "NMDC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "NTPC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "OBEROIRLTY\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "OFSS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ONGC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PAGEIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PERSISTENT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PETRONET\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PFC\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PIDILITIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PIIND\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PNB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "POLYCAB\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "POWERGRID\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "PVR\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "RAIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "RAMCOCEM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "RBLBANK\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "RECLTD\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "RELIANCE\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SAIL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SBICARD\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SBILIFE\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SBIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SHREECEM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SHRIRAMFIN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SIEMENS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SRF\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SUNPHARMA\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SUNTV\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "SYNGENE\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TATACHEM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TATACOMM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TATACONSUM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TATAMOTORS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TATAPOWER\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TATASTEEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TCS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TECHM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TITAN\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TORNTPHARM\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TRENT\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "TVSMOTOR\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "UBL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ULTRACEMCO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "UPL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "VEDL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "VOLTAS\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "WHIRLPOOL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "WIPRO\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ZEEL\n",
      "-I\n",
      "-II\n",
      "-III\n",
      "ZYDUSLIFE\n",
      "-I\n",
      "-II\n",
      "-III\n"
     ]
    }
   ],
   "source": [
    "hyphens = ['-I', '-II', '-III']\n",
    "numberOfRowsFinal = 0\n",
    "\n",
    "for symbol in tqdm(symbols):\n",
    "    print(symbol)  \n",
    "    sym1 = re.sub('[^a-zA-Z0-9 \\n\\.]', '', symbol)\n",
    "    df_final = pd.DataFrame()\n",
    "    \n",
    "    for i in hyphens:    \n",
    "        print(i)\n",
    "        file_path = final_data_path + '\\\\' + symbol + i + '.csv'\n",
    "        \n",
    "        if os.path.isfile(file_path):\n",
    "            \n",
    "            df = pd.read_csv(file_path, dayfirst=True)\n",
    "\n",
    "            df['Date'] = pd.to_datetime(df['Date'].astype(str), dayfirst=True).dt.date\n",
    "            df = df.drop(['Date'], axis=1)\n",
    "            df = df.rename(columns={'New_date' : 'Date'})\n",
    "            \n",
    "            df['Adj_Open'] = df['Open']\n",
    "            df['Adj_High'] = df['High']\n",
    "            df['Adj_Low'] = df['Low']\n",
    "            df['Adj_Close'] = df['Close']\n",
    "            df['Adj_strike'] = df['Strike']\n",
    "            df['Adj_volume'] = df['Volume']\n",
    "            df['Adj_OI'] = df['Open Interest']\n",
    "\n",
    "            df['Final_strike'] = df['Strike']\n",
    "            df['month'] = pd.to_datetime(df['Date'], dayfirst=True).dt.month    \n",
    "            \n",
    "            df = df.sort_values(by=['Date', 'Timestamp', 'Option_type', 'Strike'])\n",
    "            df = df[['New_date_Time', 'Ticker', 'Date', 'Open', 'High', 'Low', 'Close',\n",
    "                     'Volume', 'Open Interest', 'Symbol', 'Option_type', 'Temp', 'Exp_year',\n",
    "                     'Exp_month', 'Strike', 'exp_month_number', 'current_month_number',\n",
    "                     'difference', 'curr_exp_date', 'current_exp_month_number',\n",
    "                     'Diff_months', 'Timestamp', 'Adj_Open', 'Adj_High', 'Adj_Low',\n",
    "                     'Adj_Close', 'Adj_volume', 'Adj_OI', 'Adj_strike', 'Final_strike',\n",
    "                     'month']]\n",
    "            numberOfRowsFinal += df.shape[0]\n",
    "            df.to_csv(output_path + symbol + i + '.csv', mode='a', header=not os.path.exists(output_path + symbol + i + '.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22a68ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "624978\n",
      "624978\n"
     ]
    }
   ],
   "source": [
    "print(numberOfRowsRaw)\n",
    "print(numberOfRowsFinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9781bacc",
   "metadata": {},
   "source": [
    "## Right Issue Adjustment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92871b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rightIssueFile = r\"E:\\sourav\\Database\\Codes\\CsvFiles\\RightsIssue.xlsx\"\n",
    "input_path = r\"D:\\Sourav\\Data\\UpdatedTill23Dec2022\\\\\"\n",
    "#final_data_path = r\"E:\\sourav\\Database\\RoundOffDone\\\\\"\n",
    "final_data_path = input_path\n",
    "output_path = r\"E:\\sourav\\Database\\RigthIssue\\\\\"\n",
    "bhavcopy_path = r\"\\\\iMAC2\\F\\All Databases\\Options (Updated as of 29122022)\\Index Options\\Excel\\EQBhavcopyNSE\\\\\"\n",
    "\n",
    "for file in next(os.walk(output_path))[2]:\n",
    "    os.remove(output_path + file)\n",
    "\n",
    "x = pd.read_excel(rightIssueFile)\n",
    "x = x.loc[:, ~x.columns.str.contains('^Unnamed')] \n",
    "x['Ex. Date'] = pd.to_datetime(x['Ex. Date'], dayfirst=True)\n",
    "startDate = datetime.strptime(str(startDate), '%Y-%m-%d')\n",
    "endDate = datetime.strptime(str(endDate), '%Y-%m-%d')\n",
    "x = x[(x['Ex. Date'] >= startDate) & (x['Ex. Date']<=endDate)]\n",
    "x = x.sort_values(by=['Ex. Date'])\n",
    "print(x)\n",
    "\n",
    "for index, row in tqdm(x.iterrows()):\n",
    "    \n",
    "    date1 = row['Ex. Date']\n",
    "    year1 = str(date1.year)\n",
    "    month1 = date1.strftime('%b').upper()\n",
    "    day1 = date1.strftime('%d')\n",
    "    file_path = bhavcopy_path + '_' + year1 + '//' + '_' + month1 + '//' + '_' +  day1 + month1 + year1 + '.csv'\n",
    "    \n",
    "    df1 = pd.read_csv(file_path)\n",
    "    df1 = df1[df1['SYMBOL']==row['Symbol']]\n",
    "    \n",
    "    x.loc[index, 'EQ'] = df1.iloc[0]['PREVCLOSE']\n",
    "\n",
    "print(x)\n",
    "#x['Symbol_New'] = x['Symbol'].str.replace('\\_|\\&|\\-','')\n",
    "x['Symbol_New'] = x['Symbol'].copy()\n",
    "x['A'] = x['Rights Ratio'].str.replace(\"\\'\", '').str.split(':').str[0].astype(float)\n",
    "x['B'] = x['Rights Ratio'].str.replace(\"\\'\", '').str.split(':').str[1].astype(float)\n",
    "x = x[['Symbol_New', 'A', 'B', 'Rights Ratio', 'Premium', 'FACE VALUE', 'Ex. Date', 'EQ']]\n",
    "x['Expected_EQ'] = ((x['B'] * x['EQ']) + x['A'] * (x['Premium'] + x['FACE VALUE'])) / (x['A'] + x['B'])\n",
    "x['Split_Ratio'] = x['EQ'] / x['Expected_EQ']\n",
    "\n",
    "\n",
    "split_dict = {}\n",
    "for name, group in x.groupby(['Symbol_New']):\n",
    "    split_dict[name] = group\n",
    "\n",
    "symbols = next(os.walk(input_path))[2]\n",
    "for i in range(len(symbols)):\n",
    "    symbols[i] = symbols[i].replace('-III.csv', '').replace('-II.csv', '').replace('-I.csv', '')\n",
    "\n",
    "symbols = sorted(list(set(symbols)))\n",
    "print(len(symbols))\n",
    "\n",
    "symbols = sorted(list(set(symbols).intersection(x['Symbol_New'].unique())))\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0cf4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start time : ', datetime.now())\n",
    "hyphens = ['-I', '-II', '-III']\n",
    "if symbols:\n",
    "    for symbol in tqdm(symbols):\n",
    "        print(symbol)  \n",
    "        sym1 = symbol\n",
    "\n",
    "        for i in hyphens:    \n",
    "            print(i)\n",
    "            file_path = final_data_path + '//' + symbol + i + '.csv'\n",
    "            if os.path.isfile(file_path):\n",
    "\n",
    "                df = pd.read_csv(file_path)\n",
    "\n",
    "                df['Date'] = pd.to_datetime(df['Date'].astype(str), dayfirst=True).dt.date\n",
    "                df['Adj_strike'] = df['Final_strike']\n",
    "\n",
    "                if symbol in split_dict.keys():\n",
    "                    print('Symbol has some adjustment!!!')\n",
    "\n",
    "                    for j in range(split_dict[sym1].shape[0]):\n",
    "                        if j==0:\n",
    "                            df['Adj_Open'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                        df['Adj_Open'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_Open'])\n",
    "\n",
    "                            df['Adj_High'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                        df['Adj_High'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_High'])\n",
    "\n",
    "                            df['Adj_Low'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                        df['Adj_Low'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_Low'])\n",
    "\n",
    "                            df['Adj_Close'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                        df['Adj_Close'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_Close'])\n",
    "\n",
    "                            df['Adj_strike'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                        df['Adj_strike'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_strike'])\n",
    "\n",
    "                            df['Adj_volume'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                        df['Adj_volume'] * split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_volume'])\n",
    "\n",
    "                            df['Adj_OI'] = df['Adj_OI'].astype('float')\n",
    "                            df['Adj_OI'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                        df['Adj_OI'] * split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_OI'])\n",
    "\n",
    "                else:\n",
    "                    print('Symbol does not have any adjustment!!!')\n",
    "\n",
    "\n",
    "                df['Final_strike'] = df['Adj_strike']\n",
    "                df['month'] = pd.to_datetime(df['Date'], dayfirst=True).dt.month    \n",
    "\n",
    "                df.to_csv(output_path + symbol + i + '.csv', index=False)\n",
    "print('End time : ', datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063e8436",
   "metadata": {},
   "source": [
    "## Split and Bonus Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156ccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitAndBonusFile = r\"E:\\sourav\\Database\\Codes\\CsvFiles\\AllAdjustments.csv\"\n",
    "input_path = r\"D:\\Sourav\\Data\\UpdatedTill23Dec2022\\\\\"\n",
    "final_data_path = input_path\n",
    "output_path = r\"E:\\sourav\\Database\\SpliAndBonus\\\\\"\n",
    "\n",
    "for file in next(os.walk(output_path))[2]:\n",
    "    os.remove(output_path + file)\n",
    "\n",
    "x = pd.read_csv(splitAndBonusFile)\n",
    "x = x.loc[:, ~x.columns.str.contains('^Unnamed')] \n",
    "x['Ex. Date'] = pd.to_datetime(x['Ex. Date'], dayfirst=True)\n",
    "startDate = datetime.strptime(str(startDate), '%Y-%m-%d')\n",
    "endDate = datetime.strptime(str(endDate), '%Y-%m-%d')\n",
    "\n",
    "x = x[(x['Ex. Date'] >= startDate) & (x['Ex. Date']<=endDate)]\n",
    "x = x.sort_values(by=['Ex. Date'])\n",
    "print(x)\n",
    "\n",
    "x['Symbol_New'] = x['Symbol'].copy()\n",
    "x = x[x['Corporate Action'].isin(['Split', 'Bonus', 'Split ', 'split'])]\n",
    "x['Numerator'] = x['NSE Ratio'].str.replace(\"\\'\", '').str.split(':').str[0].astype(float)\n",
    "x['Denominator'] = x['NSE Ratio'].str.replace(\"\\'\", '').str.split(':').str[1].astype(float)\n",
    "x['Split_Ratio'] = np.where(x['Corporate Action']=='Bonus', 1 + (x['Numerator']/x['Denominator']), x['Numerator']/x['Denominator'])\n",
    "x = x[['Symbol_New', 'Ex. Date', 'Corporate Action', 'NSE Ratio', 'Split_Ratio']]\n",
    "\n",
    "split_dict = {}\n",
    "for name, group in x.groupby(['Symbol_New']):\n",
    "    split_dict[name] = group\n",
    "\n",
    "symbols = next(os.walk(input_path))[2]\n",
    "for i in range(len(symbols)):\n",
    "    symbols[i] = symbols[i].replace('-III.csv', '').replace('-II.csv', '').replace('-I.csv', '')\n",
    "\n",
    "symbols = sorted(list(set(symbols)))\n",
    "print(len(symbols))\n",
    "\n",
    "symbols = sorted(list(set(symbols).intersection(x['Symbol_New'].unique())))\n",
    "symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2f3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start time : ', datetime.now())\n",
    "hyphens = ['-I', '-II', '-III']\n",
    "\n",
    "if symbols:\n",
    "    for symbol in tqdm(symbols):\n",
    "        print(symbol)  \n",
    "        sym1 = symbol\n",
    "\n",
    "        for i in hyphens:    \n",
    "            print(i)\n",
    "            file_path = final_data_path + '//' + symbol + i + '.csv'\n",
    "            if os.path.isfile(file_path):\n",
    "\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Date'] = pd.to_datetime(df['Date'].astype(str), dayfirst=True).dt.date\n",
    "                df['Adj_strike'] = df['Final_strike']\n",
    "\n",
    "                if sym1 in split_dict.keys():\n",
    "\n",
    "                    for j in range(split_dict[sym1].shape[0]):\n",
    "\n",
    "                        df['Adj_Open'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_Open'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_Open'])\n",
    "\n",
    "                        df['Adj_High'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_High'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_High'])\n",
    "\n",
    "                        df['Adj_Low'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_Low'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_Low'])\n",
    "\n",
    "                        df['Adj_Close'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_Close'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_Close'])\n",
    "\n",
    "                        df['Adj_strike'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_strike'] / split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_strike'])\n",
    "\n",
    "                        df['Adj_volume'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_volume'] * split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_volume'])\n",
    "\n",
    "                        df['Adj_OI'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_OI'] * split_dict[sym1].iloc[j]['Split_Ratio'], df['Adj_OI'])\n",
    "\n",
    "                df['Final_strike'] = df['Adj_strike']\n",
    "                df['month'] = pd.to_datetime(df['Date'], dayfirst=True).dt.month    \n",
    "\n",
    "                df = df.sort_values(by=['Date', 'Option_type', 'Strike'])\n",
    "                df.to_csv(output_path + symbol + i + '.csv', index=False)\n",
    "\n",
    "print('End time : ', datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b10bd7",
   "metadata": {},
   "source": [
    "## Dividend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dividendFile = r\"E:\\sourav\\Database\\Codes\\CsvFiles\\DividendAdjustments.xlsx\"\n",
    "#input_path = r\"D:\\Sourav\\Data\\UpdatedTill23Dec2022\\\\\"\n",
    "input_path = r\"D:\\Sourav\\Data\\UpdatedTill23Dec2022 - 1\\\\\"\n",
    "final_data_path = input_path\n",
    "output_path = r\"E:\\sourav\\Database\\Dividend\\\\\"\n",
    "\n",
    "for file in next(os.walk(output_path))[2]:\n",
    "    os.remove(output_path + file)\n",
    "\n",
    "x = pd.read_excel(dividendFile)\n",
    "x = x.loc[:, ~x.columns.str.contains('^Unnamed')] \n",
    "x['Ex. Date'] = pd.to_datetime(x['Ex. Date'], dayfirst=True)\n",
    "startDate = datetime.strptime(str(startDate), '%Y-%m-%d')\n",
    "endDate = datetime.strptime(str(endDate), '%Y-%m-%d')\n",
    "x = x[(x['Ex. Date'] >= startDate) & (x['Ex. Date']<=endDate)]\n",
    "x = x.sort_values(by=['Ex. Date'])\n",
    "print(x)\n",
    "\n",
    "x['Symbol_New'] = x['Symbol'].copy()\n",
    "x = x[['Symbol_New', 'Ex. Date', 'Corporate Action', 'Dividend']]\n",
    "split_dict = {}\n",
    "for name, group in x.groupby(['Symbol_New']):\n",
    "    split_dict[name] = group\n",
    "\n",
    "\n",
    "symbols = next(os.walk(input_path))[2]\n",
    "for i in range(len(symbols)):\n",
    "    symbols[i] = symbols[i].replace('-III.csv', '').replace('-II.csv', '').replace('-I.csv', '')\n",
    "\n",
    "symbols = sorted(list(set(symbols)))\n",
    "print(len(symbols))\n",
    "\n",
    "symbols = sorted(list(set(symbols).intersection(x['Symbol_New'].unique())))\n",
    "print(symbols)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8257a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start time : ', datetime.now())\n",
    "hyphens = ['-I', '-II', '-III']\n",
    "if symbols:\n",
    "    for symbol in tqdm(symbols):\n",
    "        print(symbol)  \n",
    "        sym1 = symbol\n",
    "        for i in hyphens:    \n",
    "            print(i)\n",
    "            file_path = final_data_path + '//' + symbol + i + '.csv'\n",
    "            if os.path.isfile(file_path):\n",
    "\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Date'] = pd.to_datetime(df['Date'].astype(str), dayfirst=True).dt.date\n",
    "                df['Adj_strike'] = df['Final_strike']\n",
    "\n",
    "                if sym1 in split_dict.keys():\n",
    "\n",
    "                    for j in range(split_dict[sym1].shape[0]):\n",
    "\n",
    "                        df['Adj_strike'] = np.where(df['Date'] < split_dict[sym1].iloc[j]['Ex. Date'], \n",
    "                                                    df['Adj_strike'] - split_dict[sym1].iloc[j]['Dividend'], df['Adj_strike'])            \n",
    "\n",
    "                df['Final_strike'] = df['Adj_strike']\n",
    "                df['month'] = pd.to_datetime(df['Date'], dayfirst=True).dt.month    \n",
    "\n",
    "                df = df.sort_values(by=['Date', 'Option_type', 'Strike'])\n",
    "                df.to_csv(output_path + symbol + i + '.csv', index=False)\n",
    "\n",
    "\n",
    "print('End time : ', datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf41ce",
   "metadata": {},
   "source": [
    "## Strike Round Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c3144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.now())\n",
    "import math\n",
    "hyphens = ['-I', '-II', '-III']\n",
    "#symbol = 'IRB'\n",
    "input_path1 = r\"E:\\sourav\\Database\\SpliAndBonus\\\\\"\n",
    "input_path2 = r\"E:\\sourav\\Database\\RightIssue\\\\\"\n",
    "input_path3 = r\"E:\\sourav\\Database\\Dividend\\\\\"\n",
    "output_path = r\"E:\\sourav\\Database\\RoundOffDone\\\\\"\n",
    "\n",
    "for file in next(os.walk(output_path))[2]:\n",
    "    os.remove(output_path + file)\n",
    "    \n",
    "symbols = next(os.walk(input_path1))[2] + next(os.walk(input_path1))[2] + next(os.walk(input_path1))[2]\n",
    "\n",
    "for symbol in symbols:\n",
    "    for i in tqdm(hyphens):\n",
    "        if os.path.isfile(input_path1 + symbol + i + '.csv'):\n",
    "            df = pd.read_csv(input_path1 + symbol + i + '.csv')\n",
    "        if os.path.isfile(input_path2 + symbol + i + '.csv'):\n",
    "            df = pd.read_csv(input_path2 + symbol + i + '.csv')\n",
    "        if os.path.isfile(input_path3 + symbol + i + '.csv'):\n",
    "            df = pd.read_csv(input_path3 + symbol + i + '.csv')\n",
    "        if df.empty:\n",
    "            continue\n",
    "        print(symbol + i + ' Number of Rows : ' + str(df.shape[0]))\n",
    "        print(df)\n",
    "\n",
    "        df['Final_strike'] = df['Adj_strike'].apply(lambda x : round(x / 0.05) * 0.05)\n",
    "        df['Temp1'] = df['Final_strike']%df['Final_strike'].astype(int)\n",
    "        df['Final_strike'] = np.where(df['Temp1']==0, df['Final_strike'].astype(int), df['Final_strike'])\n",
    "        df = df.drop(['Temp1'], axis=1)\n",
    "        print(symbol + i + ' Number of Rows : ' + str(df.shape[0]))\n",
    "        df.to_csv(output_path + symbol + i + '.csv', index=False)\n",
    "\n",
    "        print(df['Final_strike'].unique())\n",
    "print(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
